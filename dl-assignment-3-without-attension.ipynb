{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:12:33.187567Z","iopub.status.busy":"2024-04-24T07:12:33.187191Z","iopub.status.idle":"2024-04-24T07:12:38.671156Z","shell.execute_reply":"2024-04-24T07:12:38.670194Z","shell.execute_reply.started":"2024-04-24T07:12:33.187539Z"},"trusted":true},"outputs":[],"source":["import os\n","import wandb\n","import torch\n","import torch.nn as nn\n","import random\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as Function\n","import argparse\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:14:11.800650Z","iopub.status.busy":"2024-04-24T07:14:11.799999Z","iopub.status.idle":"2024-04-24T07:14:11.828616Z","shell.execute_reply":"2024-04-24T07:14:11.827611Z","shell.execute_reply.started":"2024-04-24T07:14:11.800598Z"},"trusted":true},"outputs":[],"source":["SYMBOL_BEGIN, SYMBOL_END, SYMBOL_UNKNOWN, SYMBOL_PADDING = 0, 1, 2, 3\n","\n","INPUT_LABEL = \"input\"\n","TARGET_LABEL = \"target\"\n","DELIMETER = \",\"\n","\n","RNN_KEY = \"RNN\"\n","GRU_KEY = \"GRU\"\n","LSTM_KEY = \"LSTM\"\n","\n","INPUT_LANG_KEY = \"input_lang\"\n","OUTPUT_LANG_KEY = \"output_lang\"\n","PAIRS_KEY = \"pairs\"\n","MAX_LEN_KEY = \"max_len\"\n","\n","input_lang = \"eng\"\n","TARGET_LANG = \"hin\"\n","\n","TRAIN_LABEL = \"train\"\n","TEST_LABEL = \"test\"\n","VALID_LABEL = \"valid\"\n","\n","DEFAULT_PATH = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled\"\n","TRAIN_DATASET_PATH = f\"{DEFAULT_PATH}/{TARGET_LANG}/{TARGET_LANG}_{TRAIN_LABEL}.csv\"\n","VALIDATION_DATASET_PATH = f\"{DEFAULT_PATH}/{TARGET_LANG}/{TARGET_LANG}_{VALID_LABEL}.csv\"\n","TEST_DATASET_PATH = f\"{DEFAULT_PATH}/{TARGET_LANG}/{TARGET_LANG}_{TEST_LABEL}.csv\"\n","\n","NADAM_KEY = \"Nadam\"\n","\n","# Sweep param labels\n","EMBEDDING_SIZE_KEY = \"embedding_size\"\n","EPOCHS_KEY = \"epochs\"\n","ENCODER_LAYER_KEY = \"encoder_layers\"\n","DECODER_LAYER_KEY = \"decoder_layers\"\n","HIDDEN_LAYER_KEY = \"hidden_layer\"\n","IS_BIDIRECTIONAL_KEY = \"bidirectional\"\n","DROPOUT_KEY = \"dropout\"\n","CELL_TYPE_KEY = \"cell_type\"\n","LEARNING_RATE_KEY = \"learning_rate\"\n","BATCH_SIZE_KEY = \"batch_size\"\n","\n","# wandb constants\n","WANDB_PROJECT_NAME=\"dl-assignment-3\"\n","WANDB_ENTITY_NAME=\"cs23m007\"\n","\n","# wandb plot titles\n","TRAIN_ACCURACY_TITLE = \"train_acc\"\n","VALIDATION_ACCURACY_TITLE = \"val_acc\"\n","TEST_ACCURACY_TITLE = \"test_acc\"\n","TRAIN_LOSS_TITLE = \"train_loss\"\n","VALIDATION_LOSS_TITLE = \"val_loss\"\n","TEST_LOSS_TITLE = \"test_loss\"\n","\n","best_params = {\n","    EMBEDDING_SIZE_KEY :256,\n","    EPOCHS_KEY :5,\n","    ENCODER_LAYER_KEY :2,\n","    DECODER_LAYER_KEY :2,\n","    HIDDEN_LAYER_KEY :256,\n","    IS_BIDIRECTIONAL_KEY :False,\n","    DROPOUT_KEY :0.2,\n","    CELL_TYPE_KEY :LSTM_KEY,\n","    BATCH_SIZE_KEY : 32,\n","    LEARNING_RATE_KEY: 0.001\n","\n","}\n","\n","\n","\n","\n","# Set the device type to CUDA if available, otherwise use CPU\n","is_gpu = torch.cuda.is_available()\n","if is_gpu:\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_config = {\n","    \"name\" : \"CS6910_Assignemnt3_Without Attention\",\n","    \"method\" : \"random\",\n","    'metric': {\n","        'name': VALIDATION_ACCURACY_TITLE,\n","        'goal': 'maximize'\n","    },\n","    \"parameters\" : {\n","        EMBEDDING_SIZE_KEY : {\n","          \"values\" : [16, 32, 64, 256]  \n","        },\n","        EPOCHS_KEY : {\n","            \"values\" : [5,10]\n","        },\n","        ENCODER_LAYER_KEY: {\n","            \"values\": [1,2,3]\n","        },\n","        DECODER_LAYER_KEY: {\n","            \"values\": [1,2,3]\n","        },\n","        HIDDEN_LAYER_KEY:{\n","            \"values\": [16, 32, 64, 256]\n","        },\n","        IS_BIDIRECTIONAL_KEY:{\n","            \"values\": [True, False]\n","        },\n","        DROPOUT_KEY: {\n","            \"values\": [0,0.2,0.3]       \n","        }, \n","        CELL_TYPE_KEY: {\n","            \"values\": [RNN_KEY,GRU_KEY,LSTM_KEY]       \n","        },\n","        LEARNING_RATE_KEY:{\n","            \"values\":[0.001,0.01]\n","        },\n","        BATCH_SIZE_KEY:{\n","            \"values\":[32,64,128] \n","        }\n","    }\n","}"]},{"cell_type":"markdown","metadata":{},"source":["# Utility Functions and classes"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:14:13.654369Z","iopub.status.busy":"2024-04-24T07:14:13.653678Z","iopub.status.idle":"2024-04-24T07:14:13.668777Z","shell.execute_reply":"2024-04-24T07:14:13.667773Z","shell.execute_reply.started":"2024-04-24T07:14:13.654332Z"},"trusted":true},"outputs":[],"source":["class Vocabulary:\n","    \"\"\"\n","    Initialize the Vocabulary object.\n","\n","    Attributes:\n","    - str_count: A dictionary to store the count of each character encountered.\n","    - int_encodding: A dictionary to map characters to integer encodings.\n","    - n_chars: An integer representing the total number of unique characters encountered.\n","    - str_encodding: A dictionary to map integer encodings back to characters.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.str_count,self.int_encodding = dict(),dict()\n","        self.n_chars = 4\n","        self.str_encodding = {0: \"<\", 1: \">\", 2: \"?\", 3: \".\"}\n","\n","    def addWord(self, word):\n","        \"\"\"\n","        Add a word to the vocabulary.\n","\n","        Parameters:\n","        - word: A string representing the word to be added to the vocabulary.\n","        \"\"\"\n","\n","\n","        for char in word:\n","            try:\n","                self.str_count[char] += 1\n","            except:\n","                self.int_encodding[char] = self.n_chars\n","                self.str_encodding[self.n_chars] = char\n","                self.str_count[char] = 1\n","                self.n_chars += 1\n","\n","def prepareData(dir):\n","    \"\"\"\n","    Prepare data for training a sequence-to-sequence model.\n","\n","    Parameters:\n","    - dir: A string representing the directory path of the data file.\n","           The data file is expected to be in CSV format with two columns:\n","           one for input sequences and another for target sequences.\n","\n","    Returns:\n","    - input_lang: An instance of the Vocabulary class containing the vocabulary\n","                  for the input sequences.\n","    - output_lang: An instance of the Vocabulary class containing the vocabulary\n","                   for the target sequences.\n","    - pairs: A list of tuples representing input-target pairs extracted from the data.\n","    - max_len: An integer representing the maximum sequence length among input and\n","               target sequences in the dataset.\n","    \"\"\"\n","\n","    data = pd.read_csv(dir, sep=DELIMETER, names=[INPUT_LABEL, TARGET_LABEL])\n","\n","    max_input_length = data[INPUT_LABEL].apply(len).max()\n","    max_target_length = data[TARGET_LABEL].apply(len).max()\n","    \n","    max_len=max(max_input_length,max_target_length)\n","\n","    input_lang, output_lang = Vocabulary(), Vocabulary()\n","\n","    pairs = pd.concat([data[INPUT_LABEL], data[TARGET_LABEL]], axis=1).values.tolist()\n","\n","    for pair in pairs:\n","        input_lang.addWord(pair[0])\n","        output_lang.addWord(pair[1])\n","\n","    return input_lang,output_lang,pairs,max_len\n","\n","\n","def helpTensor(lang, word, max_length):\n","    \"\"\"\n","    Convert a word into a PyTorch tensor of character indexes according to a provided language mapping,\n","    padding it to a specified maximum length.\n","\n","    Parameters:\n","    - lang (dict): A dictionary mapping characters to their corresponding indexes in the language.\n","    - word (str): The input word to be converted into a tensor.\n","    - max_length (int): The maximum length of the tensor after padding.\n","\n","    Returns:\n","    - result (torch.Tensor): A PyTorch tensor containing the indexes of characters in the word, \n","      padded with SYMBOL_PADDING up to the max_length, and terminated with SYMBOL_END.\n","    \"\"\"\n","\n","    index_list = []\n","    for char in word:\n","        try:\n","            index_list.append(lang.char2index[char])\n","        except:\n","            index_list.append(SYMBOL_UNKNOWN)\n","\n","    indexes = index_list\n","    indexes.append(SYMBOL_END)\n","    n = len(indexes)\n","    indexes.extend([SYMBOL_PADDING] * (max_length - n))\n","    result = torch.LongTensor(indexes)\n","    if is_gpu:\n","        return result.cuda()\n","    return result\n","\n","def makeTensor(input_lang, output_lang, pairs, reach):\n","    res = [(helpTensor(input_lang, pairs[i][0], reach), helpTensor(output_lang, pairs[i][1], reach)) for i in range(len(pairs))]\n","    return res\n","\n","\n","def accuracy(encoder, decoder, loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang):\n","    \"\"\"\n","    Calculate the accuracy of a sequence-to-sequence model on a given dataset.\n","\n","    Args:\n","    - encoder (torch.nn.Module): The encoder module of the sequence-to-sequence model.\n","    - decoder (torch.nn.Module): The decoder module of the sequence-to-sequence model.\n","    - loader (torch.utils.data.DataLoader): DataLoader containing the dataset.\n","    - batch_size (int): The batch size for processing data.\n","    - criterion: The loss criterion used during training.\n","    - cell_type (str): Type of RNN cell used in the model (e.g., LSTM_KEY).\n","    - num_layers_enc (int): Number of layers in the encoder.\n","    - max_length (int): Maximum length of input/output sequences.\n","    - output_lang: The language object representing the output language.\n","    - input_lang: The language object representing the input language.\n","    - is_test (bool): Flag indicating whether the function is used for testing.\n","\n","    Returns:\n","    - accuracy (float): The accuracy of the model on the dataset, as a percentage.\n","\n","    \"\"\"\n","\n","    with torch.no_grad():\n","        total = correct = 0\n","\n","        for batch_x, batch_y in loader:\n","            # Initialize encoder hidden state\n","            encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n","\n","            input_variable = Variable(batch_x.transpose(0, 1))\n","            target_variable = Variable(batch_y.transpose(0, 1))\n","\n","            # Check if LSTM and initialize cell state\n","            if cell_type == LSTM_KEY:\n","                encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n","                encoder_hidden = (encoder_hidden, encoder_cell_state)\n","\n","            # input_length = input_variable.size()[0]\n","            # target_length = target_variable.size()[0]\n","\n","            output = torch.LongTensor(target_variable.size()[0], batch_size)\n","\n","            # Initialize encoder outputs\n","            # encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size))\n","            # encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n","\n","            # Encoder forward pass\n","            for ei in range(input_variable.size()[0]):\n","                encoder_hidden = encoder(input_variable[ei], batch_size, encoder_hidden)[1]\n","\n","            decoder_input = Variable(torch.LongTensor([SYMBOL_BEGIN] * batch_size))\n","            if is_gpu:\n","                decoder_input = decoder_input.cuda()\n","\n","            decoder_hidden = encoder_hidden\n","\n","            # Decoder forward pass\n","            for di in range(target_variable.size()[0]):\n","                decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n","                topi = decoder_output.data.topk(1)[1]\n","                output[di], decoder_input = torch.cat(tuple(topi)), torch.cat(tuple(topi))\n","            output = output.transpose(0, 1)\n","\n","            # Calculate accuracyWithoutAttn\n","            for di in range(output.size()[0]):\n","                ignore = [SYMBOL_BEGIN, SYMBOL_END, SYMBOL_PADDING]\n","                sent = [output_lang.str_encodding[letter.item()] for letter in output[di] if letter not in ignore]\n","                y = [output_lang.str_encodding[letter.item()] for letter in batch_y[di] if letter not in ignore]\n","                if sent == y:\n","                    correct += 1\n","                total += 1\n","\n","    return (correct / total) * 100\n","\n","\n","def calc_loss(encoder, decoder, input_tensor, target_tensor, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training, teacher_forcing_ratio=0.5):\n","    \"\"\"\n","    Calculate the loss of a sequence-to-sequence model for a single batch.\n","\n","    Args:\n","    - encoder (torch.nn.Module): The encoder module of the sequence-to-sequence model.\n","    - decoder (torch.nn.Module): The decoder module of the sequence-to-sequence model.\n","    - input_tensor (torch.Tensor): Input tensor representing the source sequence.\n","    - target_tensor (torch.Tensor): Target tensor representing the target sequence.\n","    - batch_size (int): The batch size for processing data.\n","    - encoder_optimizer (torch.optim.Optimizer): Optimizer for updating encoder parameters.\n","    - decoder_optimizer (torch.optim.Optimizer): Optimizer for updating decoder parameters.\n","    - criterion: The loss criterion used during training.\n","    - cell_type (str): Type of RNN cell used in the model (e.g., LSTM_KEY).\n","    - num_layers_enc (int): Number of layers in the encoder.\n","    - max_length (int): Maximum length of input/output sequences.\n","    - is_training (bool): Flag indicating whether the function is called during training or validation.\n","    - teacher_forcing_ratio (float, optional): The probability of using teacher forcing during training. Default is 0.5.\n","\n","    Returns:\n","    - loss (float): The average loss per target length for the batch.\n","    \"\"\"\n","\n","    # Initialize the encoder hidden state\n","    output_hidden = encoder.initHidden(batch_size, num_layers_enc)\n","\n","    # Check if LSTM and initialize cell state\n","    if cell_type == LSTM_KEY:\n","        encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n","        output_hidden = (output_hidden, encoder_cell_state)\n","\n","    # Zero the gradients\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    # Get input and target sequence lengths\n","    # input_length = input_tensor.size(0)\n","    # target_length = target_tensor.size(0)\n","\n","    # Initialize loss\n","    loss = 0\n","\n","    # Encoder forward pass\n","    for ei in range(input_tensor.size(0)):\n","        output_hidden = encoder(input_tensor[ei], batch_size, output_hidden)[1]\n","\n","    # Initialize decoder input\n","    decoder_input = torch.LongTensor([SYMBOL_BEGIN] * batch_size)\n","    decoder_input = decoder_input.cuda() if is_gpu else decoder_input\n","\n","    # Determine if using teacher forcing\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    # Loop over target sequence\n","    if is_training:\n","        # Training phase\n","        for di in range(target_tensor.size(0)):\n","            decoder_output, output_hidden = decoder(decoder_input, batch_size, output_hidden)\n","            decoder_input = target_tensor[di] if use_teacher_forcing else decoder_output.argmax(dim=1)\n","            loss = criterion(decoder_output, target_tensor[di]) + loss\n","    else:\n","        # Validation phase\n","        with torch.no_grad():\n","            for di in range(target_tensor.size(0)):\n","                decoder_output, output_hidden = decoder(decoder_input, batch_size, output_hidden)\n","                loss += criterion(decoder_output, target_tensor[di])\n","                decoder_input = decoder_output.argmax(dim=1)\n","\n","    # Backpropagation and optimization in training phase\n","    if is_training:\n","        loss.backward()\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","    # Return the average loss per target length\n","    return loss.item() / target_tensor.size(0)\n","\n","\n","# Train and evaluate the Seq2SeqWithoutAttn model\n","def seq2seq(encoder, decoder, train_loader, val_loader, test_loader, lr, optimizer, epochs, max_length_word, num_layers_enc, output_lang,batch_size,cell_type,is_wandb):\n","    \"\"\"\n","    Calculate the loss of a sequence-to-sequence model with attention mechanism for a single batch.\n","\n","    Args:\n","    - encoder (torch.nn.Module): The encoder module of the sequence-to-sequence model.\n","    - decoder (torch.nn.Module): The decoder module of the sequence-to-sequence model.\n","    - encoder_optimizer (torch.optim.Optimizer): Optimizer for updating encoder parameters.\n","    - decoder_optimizer (torch.optim.Optimizer): Optimizer for updating decoder parameters.\n","    - input_tensor (torch.Tensor): Input tensor representing the source sequence.\n","    - target_tensor (torch.Tensor): Target tensor representing the target sequence.\n","    - criterion: The loss criterion used during training.\n","    - batch_size (int): The batch size for processing data.\n","    - cell_type (str): Type of RNN cell used in the model (e.g., LSTM_KEY).\n","    - num_layers_enc (int): Number of layers in the encoder.\n","    - max_length (int): Maximum length of input/output sequences.\n","    - is_training (bool): Flag indicating whether the function is called during training or validation.\n","    - teacher_forcing_ratio (float, optional): The probability of using teacher forcing during training. Default is 0.5.\n","\n","    Returns:\n","    - avg_loss (float): The average loss per target length for the batch.\n","\n","    \"\"\"\n","\n","    max_length = max_length_word - 1\n","    # Define the optimizer and criterion\n","    encoder_optimizer = optim.NAdam(encoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(encoder.parameters(), lr=lr)\n","    decoder_optimizer = optim.NAdam(decoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(decoder.parameters(), lr=lr)\n","    criterion = nn.NLLLoss()\n","\n","    for epoch in range(epochs):\n","        train_loss_total = 0\n","        val_loss_total = 0\n","\n","        # Training phase\n","        for batch_x, batch_y in train_loader:\n","            batch_x = Variable(batch_x.transpose(0, 1))\n","            batch_y = Variable(batch_y.transpose(0, 1))\n","            # Calculate the training loss\n","            loss = calc_loss(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=True)\n","            train_loss_total += loss\n","\n","        train_loss_avg = train_loss_total / len(train_loader)\n","        print(f\"Epoch: {epoch} | Train Loss: {train_loss_avg:.4f} |\", end=\"\")\n","\n","        # Validation phase\n","        for batch_x, batch_y in val_loader:\n","            batch_x = Variable(batch_x.transpose(0, 1))\n","            batch_y = Variable(batch_y.transpose(0, 1))\n","            # Calculate the validation loss\n","            loss = calc_loss(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=False)\n","            val_loss_total += loss\n","\n","        val_loss_avg = val_loss_total / len(val_loader)\n","        print(f\"Val Loss: {val_loss_avg:.4f} |\", end=\"\")\n","\n","\n","        train_acc = accuracy(encoder, decoder, train_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n","        train_acc /= 100\n","        print(f\"train Accuracy: {train_acc:.4%} |\", end=\"\")\n","\n","        # Calculate validation accuracyWithoutAttn\n","        val_acc = accuracy(encoder, decoder, val_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n","        val_acc /= 100\n","        print(f\"Val Accuracy: {val_acc:.4%} |\", end=\"\")\n","        \n","        test_acc = accuracy(encoder, decoder, test_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n","        test_acc /= 100\n","        print(f\"Test Accuracy: {test_acc:.4%}\")\n","        if is_wandb:\n","            wandb.log(\n","                {\n","                    TRAIN_ACCURACY_TITLE: train_acc,\n","                    VALIDATION_ACCURACY_TITLE: val_acc,\n","                    TEST_ACCURACY_TITLE: test_acc,\n","                    TRAIN_LOSS_TITLE: train_loss_avg,\n","                    VALIDATION_LOSS_TITLE: val_loss_avg,\n","                    # TEST_LOSS_TITLE: test_loss\n","                }\n","            )\n","\n","            \n"]},{"cell_type":"markdown","metadata":{},"source":["# Encoder Class"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:20:36.782415Z","iopub.status.busy":"2024-04-24T07:20:36.782044Z","iopub.status.idle":"2024-04-24T07:20:36.837554Z","shell.execute_reply":"2024-04-24T07:20:36.836502Z","shell.execute_reply.started":"2024-04-24T07:20:36.782386Z"},"trusted":true},"outputs":[],"source":["# EncoderRNNWithoutAttn\n","class EncoderRNN(nn.Module):\n","    \"\"\"\n","    Encoder module of a sequence-to-sequence model.\n","\n","    Args:\n","    - input_size (int): Size of the input vocabulary.\n","    - embedding_size (int): Size of the embedding layer.\n","    - hidden_size (int): Size of the hidden state of the RNN.\n","    - num_layers_encoder (int): Number of layers in the encoder.\n","    - cell_type (str): Type of RNN cell used in the encoder (e.g., 'LSTM', 'GRU', 'RNN').\n","    - drop_out (float): Dropout probability.\n","    - bi_directional (bool): Flag indicating whether the encoder is bidirectional.\n","\n","    Attributes:\n","    - emb_n (int): Embedding size.\n","    - hid_n (int): Hidden size.\n","    - encoder_n (int): Number of layers in the encoder.\n","    - model_key (str): Type of RNN cell used in the encoder.\n","    - is_dropout (float): Dropout probability.\n","    - is_bi_dir (bool): Flag indicating whether the encoder is bidirectional.\n","    - embedding (nn.Embedding): Embedding layer.\n","    - dropout (nn.Dropout): Dropout layer.\n","    - cell_layer (nn.Module): RNN cell layer.\n","\n","    Methods:\n","    - forward(input, batch_size, hidden): Forward pass of the encoder.\n","    - initHidden(batch_size, num_layers_enc): Initialize the hidden state of the encoder.\n","    \"\"\"\n","\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional):\n","        super(EncoderRNN, self).__init__()\n","\n","        self.emb_n = embedding_size\n","        self.hid_n = hidden_size\n","        self.encoder_n = num_layers_encoder\n","        self.model_key = cell_type\n","        self.is_dropout = drop_out\n","        self.is_bi_dir = bi_directional\n","\n","        self.embedding = nn.Embedding(input_size, self.emb_n)\n","        self.dropout = nn.Dropout(self.is_dropout)\n","\n","        cell_map = dict({RNN_KEY: nn.RNN, GRU_KEY: nn.GRU, LSTM_KEY: nn.LSTM})\n","        self.cell_layer = cell_map[self.model_key](\n","            input_size = self.emb_n,\n","            hidden_size = self.hid_n,\n","            num_layers=self.encoder_n,\n","            dropout=self.is_dropout,\n","            bidirectional=self.is_bi_dir,\n","        )\n","\n","    def forward(self, input, batch_size, hidden):\n","        \"\"\"\n","        Forward pass of the encoder.\n","\n","        Args:\n","        - input (torch.Tensor): Input tensor of shape (seq_len, batch_size).\n","        - batch_size (int): Batch size.\n","        - hidden (torch.Tensor): Initial hidden state.\n","\n","        Returns:\n","        - y_cap (torch.Tensor): Output tensor of the encoder.\n","        - hidden (torch.Tensor): Updated hidden state.\n","        \"\"\"\n","\n","        embedded = self.dropout(self.embedding(input).view(1, batch_size, -1))\n","\n","        output, hidden = self.cell_layer(embedded, hidden)\n","        return output, hidden\n","\n","    def initHidden(self, batch_size, num_layers_enc):\n","        \"\"\"\n","        Initialize the hidden state of the encoder.\n","\n","        Args:\n","        - batch_size (int): Batch size.\n","        - num_layers_enc (int): Number of layers in the encoder.\n","\n","        Returns:\n","        - torch.Tensor: Initial hidden state.\n","        \"\"\"\n","\n","        if self.is_bi_dir:\n","            weights = torch.zeros(num_layers_enc * 2 , batch_size, self.hid_n)\n","        else:\n","            weights = torch.zeros(num_layers_enc, batch_size, self.hid_n)\n","\n","        if is_gpu:\n","            return weights.cuda()\n","        return weights\n","    "]},{"cell_type":"markdown","metadata":{},"source":["# Decoder Class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DecoderRNN(nn.Module):\n","    \"\"\"\n","    Decoder module of a sequence-to-sequence model with attention mechanism.\n","\n","    Args:\n","    - hidden_size (int): Size of the hidden state of the RNN.\n","    - embedding_size (int): Size of the embedding layer.\n","    - cell_type (str): Type of RNN cell used in the decoder (e.g., 'LSTM', 'GRU', 'RNN').\n","    - num_layers_decoder (int): Number of layers in the decoder.\n","    - drop_out (float): Dropout probability.\n","    - max_length_word (int): Maximum length of a word in the input sequence.\n","    - output_size (int): Size of the output vocabulary.\n","\n","    Attributes:\n","    - hid_n (int): Hidden size.\n","    - emb_n (int): Embedding size.\n","    - model_key (str): Type of RNN cell used in the decoder.\n","    - decoder_n (int): Number of layers in the decoder.\n","    - drop_out (float): Dropout probability.\n","    - max_length_word (int): Maximum length of a word in the input sequence.\n","    - embedding (nn.Embedding): Embedding layer.\n","    - attention_layer (nn.Linear): Linear layer for attention mechanism.\n","    - attention_combine (nn.Linear): Linear layer for combining attention and embedded input.\n","    - dropout (nn.Dropout): Dropout layer.\n","    - cell_layer (nn.Module): RNN cell layer.\n","    - out (nn.Linear): Linear layer for output.\n","\n","    Methods:\n","    - forward(input, batch_size, hidden, encoder_outputs): Forward pass of the decoder with attention mechanism.\n","\n","    Note:\n","    - This class represents the decoder module of a sequence-to-sequence model with an attention mechanism.\n","    - It takes embedded input tokens, hidden states, and encoder outputs as inputs, and produces output tokens with attention weights.\n","    - The type of RNN cell (e.g., LSTM, GRU) can be specified during initialization.\n","    \"\"\"\n","\n","    def __init__(self, embedding_size, hidden_size, num_layers_decoder, cell_type, drop_out, bi_directional, output_size):\n","        super(DecoderRNN, self).__init__()\n","\n","        self.emb_n = embedding_size\n","        self.hid_n = hidden_size\n","        self.decoder_n = num_layers_decoder\n","        self.model_key = cell_type\n","        self.is_dropout = drop_out\n","        self.is_bi_dir = bi_directional\n","\n","        # Create an embedding layer\n","        self.embedding = nn.Embedding(output_size, self.emb_n)\n","        self.dropout = nn.Dropout(self.is_dropout)\n","\n","        cell_map = {RNN_KEY: nn.RNN, GRU_KEY: nn.GRU, LSTM_KEY: nn.LSTM}\n","        self.cell_layer = cell_map[self.model_key](\n","            input_size = self.emb_n,\n","            hidden_size = self.hid_n,\n","            num_layers=self.decoder_n,\n","            dropout=self.is_dropout,\n","            bidirectional=self.is_bi_dir,\n","        )\n","\n","        # Linear layer for output\n","        if self.is_bi_dir :\n","            self.out = nn.Linear(self.hid_n * 2, output_size)\n","        else:\n","            self.out = nn.Linear(self.hid_n,output_size)\n","\n","        # Softmax activation\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input, batch_size, hidden):\n","        \"\"\"\n","        Forward pass of the decoder with attention mechanism.\n","\n","        Args:\n","        - input (torch.Tensor): Input tensor of shape (1, batch_size).\n","        - batch_size (int): Batch size.\n","        - hidden (torch.Tensor): Hidden state tensor.\n","        - encoder_outputs (torch.Tensor): Encoder outputs tensor.\n","\n","        Returns:\n","        - y_cap (torch.Tensor): Output tensor of the decoder.\n","        - hidden (torch.Tensor): Updated hidden state tensor.\n","        - attention_weights (torch.Tensor): Attention weights tensor.\n","        \"\"\"\n","\n","        output = Function.relu(self.dropout(self.embedding(input).view(1, batch_size, -1)))\n","        output, hidden = self.cell_layer(output, hidden)\n","\n","        output = self.softmax(self.out(output[0]))\n","        return output, hidden\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train Function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train(config_defaults = best_params,flag = False,is_wandb = True):\n","    \"\"\"\n","    Function to train a sequence-to-sequence model.\n","\n","    Args:\n","    - config_defaults (dict): Dictionary containing default hyperparameters.\n","    - flag (bool): Flag indicating whether to use attention mechanism.\n","    - is_wandb (bool): Flag indicating whether to use Weights & Biases logging.\n","    - is_heat_map (bool): Flag indicating whether to generate attention heatmaps.\n","\n","    Returns:\n","    - None\n","\n","    Note:\n","    - This function is responsible for training a sequence-to-sequence model based on the provided configurations.\n","    - If `flag` is True, the function trains a model with attention mechanism. Otherwise, it trains a vanilla sequence-to-sequence model without attention.\n","    - If `is_wandb` is True, the function logs the training process using Weights & Biases.\n","    - If `is_heat_map` is True, the function generates attention heatmaps for the test data.\n","    \"\"\"\n","\n","    optimizer = NADAM_KEY\n","    if flag:\n","        pass\n","    else:\n","        # Prepare training data\n","        if is_wandb:\n","            wandb.init(project=WANDB_PROJECT_NAME, entity=WANDB_ENTITY_NAME,config = config_defaults)\n","            args = wandb.config\n","            # Set the name of the run\n","\n","            wandb.run.name = 'ep-'+str(args[EPOCHS_KEY])+'-lr-'+str(args[LEARNING_RATE_KEY])+'-bs-'+str(args[BATCH_SIZE_KEY])+'-el-'+str(args[ENCODER_LAYER_KEY])+'-dl-'+str(args[DECODER_LAYER_KEY]) \\\n","                            +'-hl-'+str(args[HIDDEN_LAYER_KEY])+'-do-'+ str(args[DROPOUT_KEY])+ '-es-'+str(args[EMBEDDING_SIZE_KEY]) \\\n","                            + '-is_bd-'+str(args[IS_BIDIRECTIONAL_KEY])+'-model'+str(args[CELL_TYPE_KEY])\n","\n","        input_langs,output_langs,pairs,max_len = prepareData(TRAIN_DATASET_PATH)\n","        print(\"train:sample:\", random.choice(pairs))\n","        train_n = len(pairs)\n","        print(f\"Number of training examples: {train_n}\")\n","\n","        # Prepare validation data\n","        input_langs,output_langs,val_pairs,max_len_val = prepareData(VALIDATION_DATASET_PATH)\n","        val_n = len(val_pairs)\n","        print(\"validation:sample:\", random.choice(val_pairs))\n","        print(f\"Number of validation examples: {val_n}\")\n","\n","        # Prepare test data\n","        input_langs,output_langs,test_pairs,max_len_test = prepareData(TEST_DATASET_PATH)\n","        test_n = len(test_pairs)\n","        print(\"Test:sample:\", random.choice(test_pairs))\n","        print(f\"Number of Test examples: {test_n}\")\n","\n","        max_len = max(max_len, max(max_len_val, max_len_test)) + 4\n","        print(max_len)\n","\n","        # Convert data to tensors and create data loaders\n","        pairs = makeTensor(input_langs, output_langs, pairs, max_len)\n","        val_pairs = makeTensor(input_langs, output_langs, val_pairs, max_len)\n","        test_pairs = makeTensor(input_langs, output_langs, test_pairs, max_len)\n","\n","        train_loader = DataLoader(dataset = pairs, batch_size=config_defaults[BATCH_SIZE_KEY], shuffle=True)\n","        val_loader = DataLoader(dataset = val_pairs, batch_size=config_defaults[BATCH_SIZE_KEY], shuffle=True)\n","        test_loader = DataLoader(dataset = test_pairs, batch_size=config_defaults[BATCH_SIZE_KEY], shuffle=True)\n","\n","        # Create the encoder and decoder models\n","        encoder1 = EncoderRNN(\n","            input_size = input_langs.n_chars,\n","            embedding_size =  config_defaults[EMBEDDING_SIZE_KEY],\n","            hidden_size =  config_defaults[HIDDEN_LAYER_KEY],\n","            num_layers_encoder = config_defaults[ENCODER_LAYER_KEY],\n","            cell_type = config_defaults[CELL_TYPE_KEY],\n","            drop_out = config_defaults[DROPOUT_KEY],\n","            bi_directional = config_defaults[IS_BIDIRECTIONAL_KEY]\n","            )\n","        decoder1 = DecoderRNN(\n","            embedding_size = config_defaults[EMBEDDING_SIZE_KEY], \n","            hidden_size = config_defaults[HIDDEN_LAYER_KEY],\n","            num_layers_decoder = config_defaults[ENCODER_LAYER_KEY],\n","            cell_type = config_defaults[CELL_TYPE_KEY],\n","            drop_out = config_defaults[DROPOUT_KEY],\n","            bi_directional = config_defaults[IS_BIDIRECTIONAL_KEY], \n","            output_size = output_langs.n_chars\n","            )\n","\n","        if is_gpu:\n","            encoder1, decoder1 = encoder1.cuda(), decoder1.cuda()\n","\n","        print(\"vanilla seq2seqWithoutAttn\")\n","        # Train and evaluate the Seq2SeqWithoutAttn model\n","        seq2seq(\n","            encoder = encoder1,\n","            decoder = decoder1,\n","            train_loader = train_loader,\n","            val_loader = val_loader,\n","            test_loader = test_loader,\n","            lr = config_defaults[LEARNING_RATE_KEY],\n","            optimizer = optimizer,\n","            epochs = config_defaults[EPOCHS_KEY],\n","            max_length_word = max_len,\n","            num_layers_enc = config_defaults[ENCODER_LAYER_KEY],\n","            output_lang = output_langs,\n","            batch_size = config_defaults[BATCH_SIZE_KEY],\n","            cell_type = config_defaults[CELL_TYPE_KEY],\n","            is_wandb = is_wandb\n","            )\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_id = wandb.sweep(sweep_config, project=\"dl-assignment-3\", entity=\"cs23m007\")\n","print('sweep_id: ', sweep_id)\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4830499,"sourceId":8163901,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
