{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!wandb login "]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:12:33.187567Z","iopub.status.busy":"2024-04-24T07:12:33.187191Z","iopub.status.idle":"2024-04-24T07:12:38.671156Z","shell.execute_reply":"2024-04-24T07:12:38.670194Z","shell.execute_reply.started":"2024-04-24T07:12:33.187539Z"},"trusted":true},"outputs":[],"source":["import os\n","import wandb\n","import torch\n","import torch.nn as nn\n","import random\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as Function\n","import argparse\n","import csv\n","from matplotlib.font_manager import FontProperties\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:14:11.800650Z","iopub.status.busy":"2024-04-24T07:14:11.799999Z","iopub.status.idle":"2024-04-24T07:14:11.828616Z","shell.execute_reply":"2024-04-24T07:14:11.827611Z","shell.execute_reply.started":"2024-04-24T07:14:11.800598Z"},"trusted":true},"outputs":[],"source":["SYMBOL_BEGIN, SYMBOL_END, SYMBOL_UNKNOWN, SYMBOL_PADDING = 0, 1, 2, 3\n","\n","INPUT_LABEL = \"input\"\n","TARGET_LABEL = \"target\"\n","DELIMETER = \",\"\n","\n","RNN_KEY = \"RNN\"\n","GRU_KEY = \"GRU\"\n","LSTM_KEY = \"LSTM\"\n","\n","INPUT_LANG_KEY = \"input_lang\"\n","OUTPUT_LANG_KEY = \"output_lang\"\n","SOURCE_LANG_KEY = \"source_lang\"\n","TARGET_LANG_KEY = \"target_lang\"\n","\n","PAIRS_KEY = \"pairs\"\n","MAX_LEN_KEY = \"max_len\"\n","\n","INPUT_LANG = \"eng\"\n","TARGET_LANG = \"hin\"\n","\n","TRAIN_LABEL = \"train\"\n","TEST_LABEL = \"test\"\n","VALID_LABEL = \"valid\"\n","\n","DEFAULT_PATH = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled\"\n","TRAIN_DATASET_PATH = f\"{DEFAULT_PATH}/{TARGET_LANG}/{TARGET_LANG}_{TRAIN_LABEL}.csv\"\n","VALIDATION_DATASET_PATH = f\"{DEFAULT_PATH}/{TARGET_LANG}/{TARGET_LANG}_{VALID_LABEL}.csv\"\n","TEST_DATASET_PATH = f\"{DEFAULT_PATH}/{TARGET_LANG}/{TARGET_LANG}_{TEST_LABEL}.csv\"\n","\n","PREDICTION_WITHOUT_ATTENSION_FILE_NAME = \"predictions_vanilla.csv\"\n","PREDICTION_WITH_ATTENSION_FILE_NAME = \"predictions_attention.csv\"\n","\n","NADAM_KEY = \"Nadam\"\n","\n","# Sweep param labels\n","EMBEDDING_SIZE_KEY = \"embedding_size\"\n","EPOCHS_KEY = \"epochs\"\n","ENCODER_LAYER_KEY = \"encoder_layers\"\n","DECODER_LAYER_KEY = \"decoder_layers\"\n","HIDDEN_LAYER_KEY = \"hidden_layer\"\n","IS_BIDIRECTIONAL_KEY = \"bidirectional\"\n","DROPOUT_KEY = \"dropout\"\n","CELL_TYPE_KEY = \"cell_type\"\n","LEARNING_RATE_KEY = \"learning_rate\"\n","BATCH_SIZE_KEY = \"batch_size\"\n","\n","# wandb constants\n","WANDB_PROJECT_NAME=\"dl-assignment-3\"\n","WANDB_ENTITY_NAME=\"cs23m007\"\n","\n","# wandb plot titles\n","TRAIN_ACCURACY_TITLE = \"train_acc\"\n","VALIDATION_ACCURACY_TITLE = \"val_acc\"\n","VALIDATION_ACCURACY_CHAR_TITLE = \"val_acc_char\"\n","VALIDATION_ACCURACY_WORD_TITLE = \"val_acc_word\"\n","CORRECTLY_PREDICTED_TITLE = \"correctly_predicted\"\n","\n","TEST_ACCURACY_TITLE = \"test_acc\"\n","TRAIN_LOSS_TITLE = \"train_loss\"\n","VALIDATION_LOSS_TITLE = \"val_loss\"\n","TEST_LOSS_TITLE = \"test_loss\"\n","\n","CSV_COLUMN_ACTUAL_X = \"Actual_X\"\n","CSV_COLUMN_ACTUAL_Y = \"Actual_Y\"\n","CSV_COLUMN_PREDICETED_Y = \"Predicted_Y\"\n","\n","best_params = {\n","    EMBEDDING_SIZE_KEY :256,\n","    EPOCHS_KEY :5,\n","    ENCODER_LAYER_KEY :2,\n","    DECODER_LAYER_KEY :2,\n","    HIDDEN_LAYER_KEY :256,\n","    IS_BIDIRECTIONAL_KEY :False,\n","    DROPOUT_KEY :0.2,\n","    CELL_TYPE_KEY :LSTM_KEY,\n","    BATCH_SIZE_KEY : 32,\n","    LEARNING_RATE_KEY: 0.001\n","\n","}\n","\n","\n","\n","\n","# Set the device type to CUDA if available, otherwise use CPU\n","is_gpu = torch.cuda.is_available()\n","if is_gpu:\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_config = {\n","    \"name\" : \"CS6910_Assignemnt3_Without Attention\",\n","    \"method\" : \"random\",\n","    'metric': {\n","        'name': VALIDATION_ACCURACY_TITLE,\n","        'goal': 'maximize'\n","    },\n","    \"parameters\" : {\n","        EMBEDDING_SIZE_KEY : {\n","          \"values\" : [16, 32, 64, 256]  \n","        },\n","        EPOCHS_KEY : {\n","            \"values\" : [5,10]\n","        },\n","        ENCODER_LAYER_KEY: {\n","            \"values\": [1,2,3]\n","        },\n","        DECODER_LAYER_KEY: {\n","            \"values\": [1,2,3]\n","        },\n","        HIDDEN_LAYER_KEY:{\n","            \"values\": [16, 32, 64, 256]\n","        },\n","        IS_BIDIRECTIONAL_KEY:{\n","            \"values\": [True, False]\n","        },\n","        DROPOUT_KEY: {\n","            \"values\": [0,0.2,0.3]       \n","        }, \n","        CELL_TYPE_KEY: {\n","            \"values\": [RNN_KEY,GRU_KEY,LSTM_KEY]       \n","        },\n","        LEARNING_RATE_KEY:{\n","            \"values\":[0.001,0.01]\n","        },\n","        BATCH_SIZE_KEY:{\n","            \"values\":[32,64,128] \n","        }\n","    }\n","}\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Utility Functions and classes"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:14:13.654369Z","iopub.status.busy":"2024-04-24T07:14:13.653678Z","iopub.status.idle":"2024-04-24T07:14:13.668777Z","shell.execute_reply":"2024-04-24T07:14:13.667773Z","shell.execute_reply.started":"2024-04-24T07:14:13.654332Z"},"trusted":true},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self):\n","        self.str_count,self.int_encodding = dict(),dict()\n","        self.n_chars = 4\n","        self.str_encodding = {0: \"<\", 1: \">\", 2: \"?\", 3: \".\"}\n","\n","    def addWord(self, word):\n","        for char in word:\n","            try:\n","                self.str_count[char] += 1\n","            except:\n","                self.int_encodding[char] = self.n_chars\n","                self.str_encodding[self.n_chars] = char\n","                self.str_count[char] = 1\n","                self.n_chars += 1\n","\n","# prepareDataWithoutAttn\n","def prepareData(dir):\n","    data = pd.read_csv(dir, sep=DELIMETER, names=[INPUT_LABEL, TARGET_LABEL])\n","\n","    max_input_length = data[INPUT_LABEL].apply(len).max()\n","    max_target_length = data[TARGET_LABEL].apply(len).max()\n","    \n","    max_len=max(max_input_length,max_target_length)\n","\n","    input_lang, output_lang = Vocabulary(), Vocabulary()\n","\n","    pairs = pd.concat([data[INPUT_LABEL], data[TARGET_LABEL]], axis=1).values.tolist()\n","\n","    for pair in pairs:\n","        input_lang.addWord(pair[0])\n","        output_lang.addWord(pair[1])\n","\n","    return input_lang,output_lang,pairs,max_len\n","\n","\n","\n","def prepareDataWithAttention(dir):\n","\n","    data = pd.read_csv(dir, sep=DELIMETER, names=[INPUT_LABEL, TARGET_LABEL])\n","\n","    max_input_length = data[INPUT_LABEL].apply(len).max()\n","    max_target_length = data[TARGET_LABEL].apply(len).max()\n","    \n","\n","    input_lang, output_lang = Vocabulary(), Vocabulary()\n","    \n","\n","    pairs = pd.concat([data[INPUT_LABEL], data[TARGET_LABEL]], axis=1).values.tolist()\n","\n","    for pair in pairs:\n","        input_lang.addWord(pair[0])\n","        output_lang.addWord(pair[1])\n","\n","    prepared_data = {\n","        \"input_lang\": input_lang,\n","        \"output_lang\": output_lang,\n","        \"pairs\": pairs,\n","        \"max_input_length\": max_input_length,\n","        \"max_target_length\": max_target_length,\n","    }\n","\n","    return input_lang, output_lang, pairs, max_input_length, max_target_length\n","\n","def writeToCSV(actual_X,actual_Y,predicted_Y,file_name):\n","\n","    table_r = [[''.join(actual_X[i]),''.join(actual_Y[i]),''.join(predicted_Y[i])] for i in range(len(predicted_Y))]\n","    fields = [CSV_COLUMN_ACTUAL_X,CSV_COLUMN_ACTUAL_Y,CSV_COLUMN_PREDICETED_Y]\n","\n","    # writing to csv file \n","    with open(file_name, 'w') as csvfile: \n","        # creating a csv writer object \n","        csvwriter = csv.writer(csvfile) \n","            \n","        # writing the fields \n","        csvwriter.writerow(fields) \n","            \n","        # writing the data rows \n","        csvwriter.writerows(table_r)\n","\n","\n","\n","# helpTensorWithoutAttn\n","def helpTensor(lang, word, max_length):\n","    index_list = []\n","    for char in word:\n","        try:\n","            index_list.append(lang.char2index[char])\n","        except:\n","            index_list.append(SYMBOL_UNKNOWN)\n","\n","    indexes = index_list\n","    indexes.append(SYMBOL_END)\n","    n = len(indexes)\n","    indexes.extend([SYMBOL_PADDING] * (max_length - n))\n","    result = torch.LongTensor(indexes)\n","    if is_gpu:\n","        return result.cuda()\n","    return result\n","\n","\n","def helpTensorWithAttention(lang, word, max_length):\n","    index_list=[]\n","    for char in word:\n","        try:\n","            index_list.append(lang.char2index[char])\n","        except:\n","            index_list.append(SYMBOL_UNKNOWN)\n","    indexes = index_list\n","    indexes.append(SYMBOL_END)\n","    n = len(indexes)\n","    indexes.extend([SYMBOL_PADDING] * (max_length - n))\n","    result = torch.LongTensor(indexes)\n","    if is_gpu:\n","        return result.cuda()\n","    return result\n","\n","def makeTensor(input_lang, output_lang, pairs, reach):\n","    res = [(helpTensor(input_lang, pairs[i][0], reach), helpTensor(output_lang, pairs[i][1], reach)) for i in range(len(pairs))]\n","    return res\n","\n","def accuracy(encoder, decoder, loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang, input_lang,is_test):\n","    with torch.no_grad():\n","        total = correct = 0\n","        actual_X = []\n","        actual_Y = []\n","        predicted_Y = []\n","        \n","        ignore = [SYMBOL_BEGIN, SYMBOL_END, SYMBOL_PADDING]\n","        for batch_x, batch_y in loader:\n","            encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n","\n","            input_variable = Variable(batch_x.transpose(0, 1))\n","            target_variable = Variable(batch_y.transpose(0, 1))\n","\n","            if cell_type == LSTM_KEY:\n","                encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n","                encoder_hidden = (encoder_hidden, encoder_cell_state)\n","\n","\n","            output = torch.LongTensor(target_variable.size()[0], batch_size)\n","\n","            for ei in range(input_variable.size()[0]):\n","                encoder_hidden = encoder(input_variable[ei], batch_size, encoder_hidden)[1]\n","\n","            decoder_input = Variable(torch.LongTensor([SYMBOL_BEGIN] * batch_size))\n","\n","            if is_test:\n","                x = None\n","                for i in range(batch_x.size()[0]):\n","                    x = [input_lang.str_encodding[letter.item()] for letter in batch_x[i] if letter not in ignore]\n","                    actual_X.append(x)\n","\n","            if is_gpu:\n","                decoder_input = decoder_input.cuda()\n","\n","            decoder_hidden = encoder_hidden\n","\n","            # Decoder forward pass\n","            for di in range(target_variable.size()[0]):\n","                decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n","                topi = decoder_output.data.topk(1)[1]\n","                output[di], decoder_input = torch.cat(tuple(topi)), torch.cat(tuple(topi))\n","            output = output.transpose(0, 1)\n","\n","            # Calculate accuracyWithoutAttn\n","            for di in range(output.size()[0]):\n","                sent = [output_lang.str_encodding[letter.item()] for letter in output[di] if letter not in ignore]\n","                y = [output_lang.str_encodding[letter.item()] for letter in batch_y[di] if letter not in ignore]\n","                if is_test:\n","                    actual_Y.append(y)\n","                    predicted_Y.append(sent)\n"," \n","                if sent == y:\n","                    correct += 1\n","                total += 1\n","        if is_test:\n","            writeToCSV(actual_X,actual_Y,predicted_Y)\n","\n","    return (correct / total) * 100\n","\n","def accuracyWithAttention(\n","    encoder,\n","    decoder,\n","    loader,\n","    batch_size,\n","    num_layers_enc,\n","    cell_type,\n","    output_lang,\n","    input_lang,\n","    criterion,\n","    max_length,\n","    is_test=False\n","    ):\n","\n","    with torch.no_grad():\n","\n","        total = correct = 0\n","        actual_X = []\n","        actual_Y = []\n","        predicted_Y = []\n","\n","        for batch_x, batch_y in loader:\n","\n","            encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n","\n","            input_variable = Variable(batch_x.transpose(0, 1))\n","            target_variable = Variable(batch_y.transpose(0, 1))\n","\n","            if cell_type == LSTM_KEY:\n","                encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n","                encoder_hidden = (encoder_hidden, encoder_cell_state)\n","\n","\n","            output = torch.LongTensor(target_variable.size()[0], batch_size)\n","    \n","            encoder_outputs = Variable(\n","                torch.zeros(max_length, batch_size, encoder.hid_n)\n","            )\n","\n","            if is_gpu: \n","                encoder_outputs = encoder_outputs.cuda() \n","            ignore = [SYMBOL_BEGIN, SYMBOL_END, SYMBOL_PADDING]\n","\n","            if is_test:\n","                x = None\n","                for i in range(batch_x.size()[0]):\n","                    x = [input_lang.str_encodding[letter.item()] for letter in batch_x[i] if letter not in ignore]\n","                    actual_X.append(x)\n","\n","\n","            for ei in range(input_variable.size()[0]):\n","                encoder_output, encoder_hidden = encoder(\n","                    input_variable[ei], batch_size, encoder_hidden\n","                )\n","                encoder_outputs[ei] = encoder_output[0]\n","\n","            decoder_input = Variable(torch.LongTensor([SYMBOL_BEGIN] * batch_size))\n","            if is_gpu:\n","                decoder_input = decoder_input.cuda() \n","\n","            decoder_hidden = encoder_hidden\n","\n","            for di in range(target_variable.size()[0]):\n","                decoder_output, decoder_hidden, decoder_attention = decoder(\n","                    decoder_input,\n","                    batch_size,\n","                    decoder_hidden,\n","                    encoder_outputs.reshape(\n","                        batch_size, max_length, encoder.hid_n\n","                    ),\n","                )\n","                topi = decoder_output.data.topk(1)[1]\n","                decoder_input,output[di] = torch.cat(tuple(topi)),torch.cat(tuple(topi))\n","            output = output.transpose(0, 1)\n","\n","            for di in range(output.size()[0]):\n","                sent = [output_lang.str_encodding[letter.item()] for letter in output[di] if letter not in ignore]\n","                y = [output_lang.str_encodding[letter.item()] for letter in batch_y[di] if letter not in ignore]\n","\n","                if is_test:\n","                    actual_Y.append(y)\n","                    predicted_Y.append(sent)\n","\n","                if sent == y:\n","                    correct += 1\n","                total += 1\n","\n","        if is_test:\n","            writeToCSV(actual_X,actual_Y,predicted_Y,PREDICTION_WITH_ATTENSION_FILE_NAME)\n","\n","    return (correct / total) * 100\n","\n","\n","# calc_lossWithoutAttn\n","def calc_loss(encoder, decoder, input_tensor, target_tensor, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training, teacher_forcing_ratio=0.5):\n","    # Initialize the encoder hidden state\n","    output_hidden = encoder.initHidden(batch_size, num_layers_enc)\n","\n","    # Check if LSTM and initialize cell state\n","    if cell_type == LSTM_KEY:\n","        encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n","        output_hidden = (output_hidden, encoder_cell_state)\n","\n","    # Zero the gradients\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    # Get input and target sequence lengths\n","    # input_length = input_tensor.size(0)\n","    # target_length = target_tensor.size(0)\n","\n","    # Initialize loss\n","    loss = 0\n","\n","    # Encoder forward pass\n","    for ei in range(input_tensor.size(0)):\n","        output_hidden = encoder(input_tensor[ei], batch_size, output_hidden)[1]\n","\n","    # Initialize decoder input\n","    decoder_input = torch.LongTensor([SYMBOL_BEGIN] * batch_size)\n","    decoder_input = decoder_input.cuda() if is_gpu else decoder_input\n","\n","    # Determine if using teacher forcing\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    # Loop over target sequence\n","    if is_training:\n","        # Training phase\n","        for di in range(target_tensor.size(0)):\n","            decoder_output, output_hidden = decoder(decoder_input, batch_size, output_hidden)\n","            decoder_input = target_tensor[di] if use_teacher_forcing else decoder_output.argmax(dim=1)\n","            loss = criterion(decoder_output, target_tensor[di]) + loss\n","    else:\n","        # Validation phase\n","        with torch.no_grad():\n","            for di in range(target_tensor.size(0)):\n","                decoder_output, output_hidden = decoder(decoder_input, batch_size, output_hidden)\n","                loss += criterion(decoder_output, target_tensor[di])\n","                decoder_input = decoder_output.argmax(dim=1)\n","\n","    # Backpropagation and optimization in training phase\n","    if is_training:\n","        loss.backward()\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","    # Return the average loss per target length\n","    return loss.item() / target_tensor.size(0)\n","\n","def calcLossWithAttention(\n","    encoder,\n","    decoder,\n","    encoder_optimizer,\n","    decoder_optimizer,\n","    input_tensor,\n","    target_tensor,\n","    criterion,\n","    batch_size,\n","    cell_type,\n","    num_layers_enc,\n","    max_length,is_training,\n","    teacher_forcing_ratio=0.5,\n","):\n","\n","    output_hidden = encoder.initHidden(batch_size, num_layers_enc)\n","\n","    if cell_type == LSTM_KEY:\n","        encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n","        output_hidden = (output_hidden, encoder_cell_state)\n","\n","    if is_training:\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","\n","    encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hid_n))\n","    if is_gpu:\n","        encoder_outputs = encoder_outputs.cuda()\n","    loss = 0\n","\n","    for ei in range(input_length):\n","        encoder_output, output_hidden = encoder(\n","            input_tensor[ei], batch_size, output_hidden\n","        )\n","        encoder_outputs[ei] = encoder_output[0]\n","\n","    decoder_input = Variable(torch.LongTensor([SYMBOL_BEGIN] * batch_size))\n","    if is_gpu :\n","        decoder_input = decoder_input.cuda() \n","    decoder_hidden = output_hidden\n","    if is_training:\n","        use_teacher_forcing = False\n","        if random.random() < teacher_forcing_ratio:\n","            use_teacher_forcing = True\n","        n = target_length\n","        if not use_teacher_forcing :\n","            for di in range(n):\n","                decoder_output, decoder_hidden, decoder_attention = decoder(\n","                    decoder_input,\n","                    batch_size,\n","                    decoder_hidden,\n","                    encoder_outputs.reshape(batch_size, max_length, encoder.hid_n),\n","                )\n","                #2 for loop ko bhar dal de\n","                topv, topi = decoder_output.data.topk(1)\n","                decoder_input = torch.cat(tuple(topi))\n","                if is_gpu :\n","                    decoder_input = decoder_input.cuda() \n","                loss += criterion(decoder_output, target_tensor[di])\n","        else:\n","            for di in range(n):\n","                decoder_output, decoder_hidden, decoder_attention = decoder(\n","                    decoder_input,\n","                    batch_size,\n","                    decoder_hidden,\n","                    encoder_outputs.reshape(batch_size, max_length, encoder.hid_n),\n","                )\n","                loss += criterion(decoder_output, target_tensor[di])\n","                decoder_input = target_tensor[di]\n","            \n","\n","        loss.backward()\n","\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","    else :\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input,\n","                batch_size,\n","                decoder_hidden,\n","                encoder_outputs.reshape(batch_size, max_length, encoder.hid_n),\n","            )\n","            topv, topi = decoder_output.data.topk(1)\n","            decoder_input = torch.cat(tuple(topi))\n","\n","            if is_gpu:\n","                decoder_input = decoder_input.cuda()\n","            loss += criterion(decoder_output, target_tensor[di])\n","\n","    avg_loss = loss.item() / target_length\n","    return avg_loss\n","\n","\n","\n","\n","# Train and evaluate the Seq2SeqWithoutAttn model\n","def seq2seq(encoder, decoder, train_loader, val_loader, test_loader, lr, optimizer, epochs, max_length_word, num_layers_enc, output_lang, input_lang, batch_size,cell_type,is_wandb):\n","    max_length = max_length_word - 1\n","    # Define the optimizer and criterion\n","    encoder_optimizer = optim.NAdam(encoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(encoder.parameters(), lr=lr)\n","    decoder_optimizer = optim.NAdam(decoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(decoder.parameters(), lr=lr)\n","    criterion = nn.NLLLoss()\n","\n","    for epoch in range(epochs):\n","        train_loss_total = 0\n","        val_loss_total = 0\n","\n","        # Training phase\n","        for batch_x, batch_y in train_loader:\n","            batch_x = Variable(batch_x.transpose(0, 1))\n","            batch_y = Variable(batch_y.transpose(0, 1))\n","            # Calculate the training loss\n","            loss = calc_loss(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=True)\n","            train_loss_total += loss\n","\n","        train_loss_avg = train_loss_total / len(train_loader)\n","        print(f\"Epoch: {epoch} | Train Loss: {train_loss_avg:.4f} |\", end=\"\")\n","\n","        # Validation phase\n","        for batch_x, batch_y in val_loader:\n","            batch_x = Variable(batch_x.transpose(0, 1))\n","            batch_y = Variable(batch_y.transpose(0, 1))\n","            # Calculate the validation loss\n","            loss = calc_loss(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=False)\n","            val_loss_total += loss\n","\n","        val_loss_avg = val_loss_total / len(val_loader)\n","        print(f\"Val Loss: {val_loss_avg:.4f} |\", end=\"\")\n","\n","        train_acc = accuracy(\n","            encoder= encoder,\n","            decoder = decoder,\n","            loader = train_loader, \n","            batch_size = batch_size, \n","            criterion = criterion, \n","            cell_type = cell_type, \n","            num_layers_enc = num_layers_enc, \n","            max_length = max_length,\n","            output_lang = output_lang,\n","            input_lang = input_lang,\n","            is_test = True\n","            )\n","        train_acc /= 100\n","        print(f\"train Accuracy: {train_acc:.4%} |\", end=\"\")\n","\n","        # Calculate validation accuracyWithoutAttn\n","        val_acc = accuracy(\n","            encoder= encoder,\n","            decoder = decoder,\n","            loader = val_loader, \n","            batch_size = batch_size, \n","            criterion = criterion, \n","            cell_type = cell_type, \n","            num_layers_enc = num_layers_enc, \n","            max_length = max_length,\n","            output_lang = output_lang,\n","            input_lang = input_lang,\n","            is_test = True\n","            )\n","\n","        val_acc /= 100\n","        print(f\"Val Accuracy: {val_acc:.4%} |\", end=\"\")\n","\n","        test_acc = accuracy(\n","            encoder= encoder,\n","            decoder = decoder,\n","            loader = test_loader, \n","            batch_size = batch_size, \n","            criterion = criterion, \n","            cell_type = cell_type, \n","            num_layers_enc = num_layers_enc, \n","            max_length = max_length,\n","            output_lang = output_lang,\n","            input_lang = input_lang,\n","            is_test = True\n","            )        \n","        test_acc = accuracy(encoder, decoder, test_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang, input_lang)\n","        test_acc /= 100\n","        print(f\"Test Accuracy: {test_acc:.4%}\")\n","        if is_wandb:\n","            wandb.log(\n","                {\n","                    TRAIN_ACCURACY_TITLE: train_acc,\n","                    VALIDATION_ACCURACY_TITLE: val_acc,\n","                    TEST_ACCURACY_TITLE: test_acc,\n","                    TRAIN_LOSS_TITLE: train_loss_avg,\n","                    VALIDATION_LOSS_TITLE: val_loss_avg,\n","                    # TEST_LOSS_TITLE: test_loss\n","                }\n","            )\n","\n","\n","def seq2seqWithAttention(\n","    encoder,\n","    decoder,\n","    train_loader,\n","    val_loader,\n","    test_loader,\n","    learning_rate,\n","    optimizer,\n","    epochs,\n","    max_length_word,\n","    attention,\n","    num_layers_enc,\n","    output_lang,\n","    input_lang,\n","    batch_size,\n","    cell_type,\n","    is_wandb\n"," ):\n","    max_length = max_length_word - 1\n","    n_val = len(val_loader)\n","    n_train = len(train_loader)\n","    encoder_optimizer = (\n","        optim.NAdam(encoder.parameters(), lr=learning_rate)\n","        if optimizer == \"nadam\"\n","        else optim.Adam(encoder.parameters(), lr=learning_rate)\n","    )\n","    decoder_optimizer = (\n","        optim.NAdam(decoder.parameters(), lr=learning_rate)\n","        if optimizer == \"nadam\"\n","        else optim.Adam(decoder.parameters(), lr=learning_rate)\n","    )\n","    criterion = nn.NLLLoss()\n","\n","    for epoch in range(epochs):\n","        train_loss_total, val_loss_total = 0, 0\n","        \n","        for batchx, batchy in train_loader:\n","            batchx = Variable(batchx.transpose(0, 1))\n","            batchy = Variable(batchy.transpose(0, 1))\n","            loss = calcLossWithAttention(\n","                encoder = encoder,\n","                decoder =decoder,\n","                encoder_optimizer = encoder_optimizer,\n","                decoder_optimizer = decoder_optimizer,\n","                input_tensor = batchx,\n","                target_tensor = batchy,\n","                criterion = criterion,\n","                batch_size =  batch_size,\n","                cell_type = cell_type,\n","                num_layers_enc = num_layers_enc,\n","                max_length = max_length + 1,\n","                is_training = True, #is_training\n","                teacher_forcing_ratio=0.5\n","            )\n","            train_loss_total = train_loss_total + loss\n","        train_loss_avg = train_loss_total / n_train\n","        print(f\"Epoch: {epoch} | Train Loss: {train_loss_avg:.4f} | \", end=\"\")\n","\n","        for batchx, batchy in val_loader:\n","            batchx = Variable(batchx.transpose(0, 1))\n","            batchy = Variable(batchy.transpose(0, 1))\n","            loss = calcLossWithAttention(\n","                encoder = encoder,\n","                decoder = decoder,\n","                encoder_optimizer = encoder_optimizer,\n","                decoder_optimizer = decoder_optimizer,\n","                input_tensor = batchx,\n","                target_tensor = batchy,\n","                criterion = criterion,\n","                batch_size =  batch_size,\n","                cell_type = cell_type,\n","                num_layers_enc = num_layers_enc,\n","                max_length = max_length + 1,\n","                is_training = False, #is_training\n","                teacher_forcing_ratio=0.5\n","            )\n","\n","            val_loss_total = val_loss_total+ loss\n","\n","        val_loss_avg = val_loss_total / n_val\n","        print(f\"Val Loss: {val_loss_avg:.4f} | \", end=\"\")\n","\n","        train_acc = accuracyWithAttention(\n","            encoder = encoder,\n","            decoder = decoder,\n","            loader = train_loader,\n","            batch_size = batch_size,\n","            num_layers_enc = num_layers_enc,\n","            cell_type = cell_type,\n","            output_lang = output_lang,\n","            input_lang = input_lang,\n","            criterion = criterion,\n","            max_length = max_length + 1,\n","        ) \n","        train_acc = train_acc /  100\n","        print(f\"Train Accuracy: {train_acc:.4%} |\", end=\"\")\n","\n","        val_acc = accuracyWithAttention(\n","            encoder = encoder,\n","            decoder = decoder,\n","            loader = val_loader,\n","            batch_size = batch_size,\n","            num_layers_enc = num_layers_enc,\n","            cell_type = cell_type,\n","            output_lang = output_lang,\n","            input_lang = input_lang,\n","            criterion = criterion,\n","            max_length = max_length + 1,\n","        )\n","        val_acc = val_acc / 100\n","        print(f\"Val Accuracy: {val_acc:.4%} |\", end=\"\")\n","        test_acc = accuracyWithAttention(\n","            encoder = encoder,\n","            decoder = decoder,\n","            loader = test_loader,\n","            batch_size = batch_size,\n","            num_layers_enc = num_layers_enc,\n","            cell_type = cell_type,\n","            output_lang = output_lang,\n","            input_lang = input_lang,\n","            criterion = criterion,\n","            max_length = max_length + 1,\n","        )\n","\n","        test_acc = test_acc / 100\n","        print(f\"Test Accuracy: {test_acc:.4%}\")\n","\n","        if is_wandb:\n","            wandb.log(\n","                {\n","                    TRAIN_ACCURACY_TITLE: train_acc,\n","                    VALIDATION_ACCURACY_TITLE: val_acc,\n","                    TEST_ACCURACY_TITLE: test_acc,\n","                    TRAIN_LOSS_TITLE: train_loss_avg,\n","                    VALIDATION_LOSS_TITLE: val_loss_avg,\n","                    # TEST_LOSS_TITLE: test_loss\n","                }\n","            )\n","\n","def store_heatmaps(encoder, decoder, loader, give_batch_size, given_num_layers_encoder,cell_type, input_lang,max_length,bi_directional,output_lang):\n","\n","    temp = give_batch_size\n","    # Evaluating for 10 test inputs so batch size is set to 1\n","    give_batch_size = 1\n","\n","    # disabled gradient calculation for inference, helps reduce memory consumption for computations\n","    with torch.no_grad():\n","\n","        # Need heatmaps for 10 inputs only that will be ensured by count\n","        count = 0\n","        # Need to store predicted y's, x's and attentions corresponding to these 10 inputs\n","        predictions,xs,attentions = [],[],[]\n","\n","        # Loops until 10 points have been covered\n","        for batch_x, batch_y in loader:\n","            count+=1\n","            # Fetch batch size and number of layers from configuration dictionary\n","            batch_size,num_layers_enc = give_batch_size,given_num_layers_encoder\n","            # Initial encoder hidden is set to a tensor of zeros using initHidden function\n","            encoder_hidden = encoder.initHidden(batch_size,num_layers_enc)\n","\n","            decoder_attentions = torch.zeros(max_length, batch_size, max_length)\n","\n","            # Transpose the batch_x and batch_y tensors \n","            input_variable, target_variable = batch_x.transpose(0, 1), batch_y.transpose(0, 1)\n","\n","            # Incase of LSTM cell the encoder hidden is a tupple of (encoder hidden, encoder cell state) while for the other two it is just encoder hidden\n","            if cell_type == LSTM_KEY:\n","                encoder_hidden = (encoder_hidden, encoder.initHidden(batch_size,num_layers_enc))\n","\n","            # Finding the length of input and target tensors\n","            \n","            input_var_n,target_var_n = input_variable.size(),target_variable.size()\n","\n","            ip_n,op_n = input_var_n[0],target_var_n[0]\n","\n","            # Appending the input word\n","            x = None\n","            for i in range(batch_x.size()[0]):\n","                x = [input_lang.str_encodding[letter.item()] for letter in batch_x[i] if letter not in [SYMBOL_BEGIN, SYMBOL_END, SYMBOL_PADDING, SYMBOL_UNKNOWN]]\n","                xs.append(x)\n","\n","            # Initializing output as a tensor of size target_length X batch_size\n","            output = torch.LongTensor(op_n, batch_size)\n","\n","            # In attention mechanism we need encoder ouputs at every time step to be stored and so we will initialize a tensor that will be used to store them\n","            encoder_outputs = None\n","            temp_hid_size = encoder.hid_n\n","            if bi_directional :\n","                # Incase of bidirectional hidden size needs to be doubled\n","                temp_hid_size = encoder.hid_n*2\n","            encoder_outputs = Variable(torch.zeros(max_length, batch_size, temp_hid_size))\n","                # Shift encoder outputs to cuda if available\n","            if is_gpu:\n","                encoder_outputs = encoder_outputs.cuda()  \n","            # Passing ith character of every word from the input batch into the encoder iteratively  \n","            for i in range(ip_n):\n","                encoder_result = encoder(input_variable[i], batch_size, encoder_hidden)\n","\n","                encoder_output = encoder_result[0]\n","                encoder_hidden = encoder_result[1]\n","\n","                encoder_outputs[i] = encoder_output[0]\n","\n","            # Setting a list of start of word tokens to pass into the decoder initially and converting it into tensors\n","            sow_list = [SYMBOL_BEGIN] * batch_size\n","            decoder_input = torch.LongTensor(sow_list)\n","            # Shift decoder_inputs to cuda if available\n","            if is_gpu :\n","                decoder_input = decoder_input.cuda() \n","\n","            # decoder_hidden is set to the final encoder_hidden after the encoder loop completes\n","            decoder_hidden = encoder_hidden\n","            # Initialize them before using them\n","            decoder_output,decoder_attention = None,None\n","\n","            # We are just evaluating the output of decoder in this case so we don't use teacher forcing here\n","            # We give the decoder input to be the best predictions for i+1th charcter for the whole batch from ith time step\n","            for i in range(op_n):\n","                enc_hid_size =  encoder.hid_n\n","                if bi_directional :\n","                    enc_hid_size = enc_hid_size*2\n","\n","                decoder_output, decoder_hidden, decoder_attention= decoder(decoder_input, batch_size, decoder_hidden, encoder_outputs.reshape(batch_size,max_length, encoder.hid_n))\n","                # Best prediction comes from using topk(k=1) function                \n","\n","                decoder_attentions[i] = decoder_attention.data \n","                temp = decoder_output.data\n","                topi_result = temp.topk(1)\n","                decoder_input = topi_result[1]\n","                output[i] = torch.cat(tuple(topi_result[1]))\n","            \n","            # Appending the attentions for every input word\n","            attentions.append(decoder_attentions)\n","\n","            # Taking transpose of output and finding it's length\n","            output = output.transpose(0,1)\n","            op_len = output.size()\n","            sent = list()\n","            for i in range(op_len[0]):\n","                # sent is ith predicted word of a batch\n","                for letter in output[i]:\n","                    if letter not in [SYMBOL_BEGIN, SYMBOL_END, SYMBOL_PADDING, SYMBOL_UNKNOWN]:\n","                        sent.append(output_lang.str_encodding[letter.item()])\n","                # Appending the predicted words for the input word\n","                predictions.append(sent)\n","\n","            if count == 12 : \n","                give_batch_size = temp\n","                # Returns input words, predicted words, attentions respectively\n","                return predictions,attentions,xs\n","\n","def plot_heatmap(test_loader, encoder, decoder,batch_size,num_layers_encoder,cell_type,max_length_word,input_lang,output_lang,bi_directional):\n","    max_length = max_length_word\n","    # input words, predicted words, attentions respectively fetched from store_heatmaps\n","    predictions,atte,test_english = store_heatmaps(\n","        encoder = encoder,\n","        decoder=decoder,\n","        loader=test_loader,\n","        give_batch_size=batch_size,\n","        given_num_layers_encoder=num_layers_encoder,\n","        cell_type=cell_type,\n","        input_lang=input_lang,\n","        max_length=max_length+1,\n","        bi_directional=bi_directional,\n","        output_lang = output_lang\n","        )\n","    # fig will store the figure with 10 subplots\n","    heat_map_plot = []\n","    n = 12 \n","    heat_map_plot , axs = plt.subplots(4,3)\n","    heat_map_plot.set_size_inches(23, 15)\n","    l,k = -1,0\n","    # Iterate 12 times\n","    while i<n:\n","        attn_weights = []\n","        # Fetch attention corresponding to ith input word\n","        attn_weight = atte[i].reshape(-1,max_length+1)\n","        ylabel,xlabel = [\"\"],[\"\"]\n","        # ylabel will have predicted word\n","        ylabel += [char for char in predictions[i]]\n","        # xlabel will have input word\n","        xlabel += [char for char in test_english[i]]\n","        \n","        # y will be of size of ylable\n","        pred_n = len(predictions[i])+1\n","        for j in range(1,pred_n):\n","            # x will be of size of xlabel\n","            temp = len(xlabel)\n","            fg = attn_weight[j][1:temp]\n","            fg_arr = fg.numpy()\n","            attn_weights.append(fg_arr)\n","            \n","        attn_weights = attn_weights[:-1]\n","        # After every 3 goto next line\n","        if i%3 == 0:\n","            l = l + 1\n","            k = 0\n","\n","        axs[l][k].set_xticklabels(xlabel)\n","        # set ylabels with support for hindi text\n","        xyz = FontProperties(fname = \"MANGAL.TTF\", size = 10)\n","        axs[l][k].set_yticklabels(ylabel, fontproperties = xyz)\n","        k+=1\n","        i+=1\n","    # Plot on wandb\n","    run = wandb.init(project=project_name_ap,entity = entity_name_ap)\n","    plt.show()\n","    wandb.log({'heatmaps':heat_map_plot})\n","    wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["# Encoder Class"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:20:36.782415Z","iopub.status.busy":"2024-04-24T07:20:36.782044Z","iopub.status.idle":"2024-04-24T07:20:36.837554Z","shell.execute_reply":"2024-04-24T07:20:36.836502Z","shell.execute_reply.started":"2024-04-24T07:20:36.782386Z"},"trusted":true},"outputs":[],"source":["# EncoderRNNWithoutAttn\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional):\n","        super(EncoderRNN, self).__init__()\n","\n","        self.emb_n = embedding_size\n","        self.hid_n = hidden_size\n","        self.encoder_n = num_layers_encoder\n","        self.model_key = cell_type\n","        self.is_dropout = drop_out\n","        self.is_bi_dir = bi_directional\n","\n","        self.embedding = nn.Embedding(input_size, self.emb_n)\n","        self.dropout = nn.Dropout(self.is_dropout)\n","\n","        cell_map = dict({RNN_KEY: nn.RNN, GRU_KEY: nn.GRU, LSTM_KEY: nn.LSTM})\n","        self.cell_layer = cell_map[self.model_key](\n","            input_size = self.emb_n,\n","            hidden_size = self.hid_n,\n","            num_layers=self.encoder_n,\n","            dropout=self.is_dropout,\n","            bidirectional=self.is_bi_dir,\n","        )\n","\n","    def forward(self, input, batch_size, hidden):\n","        pre_embedded = self.embedding(input)\n","        transformed_embedded_data = pre_embedded.view(1, batch_size, -1)\n","        embedded = self.dropout(transformed_embedded_data)\n","        y_cap, hidden = self.cell_layer(embedded, hidden)\n","        return y_cap, hidden\n","\n","    def initHidden(self, batch_size, num_layers_enc):\n","        if self.is_bi_dir:\n","            weights = torch.zeros(num_layers_enc * 2 , batch_size, self.hid_n)\n","        else:\n","            weights = torch.zeros(num_layers_enc, batch_size, self.hid_n)\n","\n","        if is_gpu:\n","            return weights.cuda()\n","        return weights\n","    \n","\n","class EncoderRNNWithAttention(nn.Module):\n","    def __init__(self, input_size, embedding_size,hidden_size,num_layers_encoder,cell_type,drop_out,bi_directional):\n","        super(EncoderRNNWithAttention, self).__init__()\n","\n","        self.emb_n = embedding_size\n","        self.hid_n = hidden_size\n","        self.encoder_n = num_layers_encoder\n","        self.model_key = cell_type\n","        self.is_dropout = drop_out\n","        self.is_bi_dir = bi_directional\n","\n","        self.embedding = nn.Embedding(input_size, self.emb_n)\n","        self.dropout = nn.Dropout(self.is_dropout)\n","\n","        cell_map = dict({RNN_KEY: nn.RNN, GRU_KEY: nn.GRU, LSTM_KEY: nn.LSTM})\n","        self.cell_layer = cell_map[self.model_key](\n","            input_size = self.emb_n,\n","            hidden_size = self.hid_n,\n","            num_layers=self.encoder_n,\n","            dropout=self.is_dropout,\n","            bidirectional=self.is_bi_dir,\n","        )\n","\n","    def forward(self, input, batch_size, hidden):\n","        pre_embedded = self.embedding(input)\n","        transformed_embedded_data = pre_embedded.view(1, batch_size, -1)\n","        embedded = self.dropout(transformed_embedded_data)\n","        y_cap, hidden = self.cell_layer(embedded, hidden)\n","        return y_cap, hidden\n","\n","    def initHidden(self, batch_size, num_layers_enc):\n","        if self.is_bi_dir:\n","            weights = torch.zeros(num_layers_enc * 2 , batch_size, self.hid_n)\n","        else:\n","            weights = torch.zeros(num_layers_enc, batch_size, self.hid_n)\n","\n","        if is_gpu:\n","            return weights.cuda()\n","        return weights\n"," \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Decoder Class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# DecoderRNNWithoutAttn\n","class DecoderRNN(nn.Module):\n","    def __init__(self, embedding_size, hidden_size, num_layers_decoder, cell_type, drop_out, bi_directional, output_size):\n","        super(DecoderRNN, self).__init__()\n","\n","        self.emb_n = embedding_size\n","        self.hid_n = hidden_size\n","        self.decoder_n = num_layers_decoder\n","        self.model_key = cell_type\n","        self.is_dropout = drop_out\n","        self.is_bi_dir = bi_directional\n","\n","        # Create an embedding layer\n","        self.embedding = nn.Embedding(output_size, self.emb_n)\n","        self.dropout = nn.Dropout(self.is_dropout)\n","\n","        cell_map = dict({RNN_KEY: nn.RNN, GRU_KEY: nn.GRU, LSTM_KEY: nn.LSTM})\n","\n","        if self.cell_type in cell_map:\n","            self.cell_layer = cell_map[self.model_key](\n","                input_size = self.emb_n,\n","                hidden_size = self.hid_n,\n","                num_layers=self.decoder_n,\n","                dropout=self.is_dropout,\n","                bidirectional=self.is_bi_dir,\n","            )\n","\n","        # Linear layer for output\n","        if self.is_bi_dir :\n","            self.out = nn.Linear(self.hid_n * 2, output_size)\n","        else:\n","            self.out = nn.Linear(self.hid_n,output_size)\n","\n","        # Softmax activation\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input, batch_size, hidden):\n","        y_cap = Function.relu(self.dropout(self.embedding(input).view(1, batch_size, -1)))\n","        y_cap, hidden = self.cell_layer(y_cap, hidden)\n","\n","        y_cap = self.softmax(self.out(y_cap[0]))\n","        return y_cap, hidden\n","\n","\n","class DecoderRNNWithAttention(nn.Module):\n","    def __init__(\n","        self,\n","        hidden_size,\n","        embedding_size,\n","        cell_type,\n","        num_layers_decoder,\n","        drop_out,\n","        max_length_word,\n","        output_size,\n","    ):\n","\n","        super(DecoderRNNWithAttention, self).__init__()\n","\n","        self.hid_n = hidden_size\n","        self.emb_n = embedding_size\n","        self.model_key = cell_type\n","        self.decoder_n = num_layers_decoder\n","        self.drop_out = drop_out\n","        self.max_length_word = max_length_word\n","\n","        self.embedding = nn.Embedding(output_size, embedding_dim=self.emb_n)\n","        layer_size = self.emb_n + self.hid_n \n","        self.attention_layer = nn.Linear(\n","            layer_size , self.max_length_word\n","        )\n","        self.attention_combine = nn.Linear(\n","            layer_size, self.emb_n\n","        )\n","        self.dropout = nn.Dropout(self.drop_out)\n","\n","        self.cell_layer = None\n","        cell_map = dict({RNN_KEY: nn.RNN, GRU_KEY: nn.GRU, LSTM_KEY: nn.LSTM})\n","\n","        if self.model_key in cell_map:\n","            self.cell_layer = cell_map[self.model_key](\n","                self.emb_n,\n","                self.hid_n,\n","                num_layers=self.decoder_n,\n","                dropout=self.drop_out,\n","            )\n","\n","        self.out = nn.Linear(self.hid_n, output_size)\n","\n","    def forward(self, input, batch_size, hidden, encoder_outputs):\n","        pre_embedded = self.embedding(input)\n","        embedded = pre_embedded.view(1, batch_size, -1)\n","\n","        attention_weights = None\n","\n","        attention_weights = Function.softmax(\n","            self.attention_layer(torch.cat((embedded[0],hidden[0][0] if self.model_key == LSTM_KEY else hidden[0]), 1)), dim=1\n","        )\n","\n","        attention_applied = torch.bmm(\n","            attention_weights.view(batch_size, 1, self.max_length_word),\n","            encoder_outputs,\n","        ).view(1, batch_size, -1)\n","\n","        y_cap = torch.cat((embedded[0], attention_applied[0]), 1)\n","        y_cap = Function.relu(self.attention_combine(y_cap).unsqueeze(0))\n","        # if self.cell_type=RNN\" :\n","        y_cap, hidden = self.cell_layer(y_cap, hidden)\n","        y_cap = Function.log_softmax(self.out(y_cap[0]), dim=1)\n","\n","        return y_cap, hidden, attention_weights\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train Function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train(config_defaults = best_params,flag = False,is_wandb = True,is_heat_map = False):\n","    optimizer = NADAM_KEY\n","    if is_wandb:\n","        wandb.init(project=WANDB_PROJECT_NAME, entity=WANDB_ENTITY_NAME,config = config_defaults)\n","        args = wandb.config\n","        # Set the name of the run\n","\n","        wandb.run.name = 'ep-'+str(args[EPOCHS_KEY])+'-lr-'+str(args[LEARNING_RATE_KEY])+'-bs-'+str(args[BATCH_SIZE_KEY])+'-el-'+str(args[ENCODER_LAYER_KEY])+'-dl-'+str(args[DECODER_LAYER_KEY]) \\\n","                        +'-hl-'+str(args[HIDDEN_LAYER_KEY])+'-do-'+ str(args[DROPOUT_KEY])+ '-es-'+str(args[EMBEDDING_SIZE_KEY]) \\\n","                        + '-is_bd-'+str(args[IS_BIDIRECTIONAL_KEY])+'-model'+str(args[CELL_TYPE_KEY])\n","\n","    if flag:\n","\n","        input_langs, output_langs, pairs, max_input_length, max_target_length = prepareDataWithAttention(TRAIN_DATASET_PATH)\n","        print(\"train:sample:\", random.choice(pairs))\n","        print(f\"Number of training examples: {len(pairs)}\")\n","\n","        # validation\n","        _,_,val_pairs,_,_ = prepareDataWithAttention(VALIDATION_DATASET_PATH)\n","\n","        print(\"validation:sample:\", random.choice(val_pairs))\n","        print(f\"Number of validation examples: {len(val_pairs)}\")\n","        # Test\n","        _,_,test_pairs,_,_ = prepareDataWithAttention(TEST_DATASET_PATH)\n","        print(\"Test:sample:\", random.choice(test_pairs))\n","        print(f\"Number of Test examples: {len(test_pairs)}\")\n","\n","        max_len = max(max_input_length, max_target_length) + 3\n","        print(max_len)\n","\n","        pairs = makeTensor(input_lang=input_langs,output_lang= output_langs,pairs= pairs, reach=max_len)\n","        val_pairs = makeTensor(input_lang=input_langs,output_lang= output_langs,pairs= val_pairs, reach=max_len)\n","        test_pairs = makeTensor(input_lang=input_langs,output_lang= output_langs,pairs= test_pairs, reach=max_len)\n","\n","        train_loader = DataLoader(pairs, batch_size=config_defaults[BATCH_SIZE_KEY], shuffle=True)\n","        val_loader = DataLoader(val_pairs, batch_size=config_defaults[BATCH_SIZE_KEY], shuffle=True)\n","        test_loader = DataLoader(test_pairs, batch_size=config_defaults[BATCH_SIZE_KEY], shuffle=True)\n","\n","        encoder1 = EncoderRNNWithAttention(\n","            input_size = input_langs.n_chars,\n","            embedding_size =  config_defaults[EMBEDDING_SIZE_KEY],\n","            hidden_size =  config_defaults[HIDDEN_LAYER_KEY],\n","            num_layers_encoder = config_defaults[ENCODER_LAYER_KEY],\n","            cell_type = config_defaults[CELL_TYPE_KEY],\n","            drop_out = config_defaults[DROPOUT_KEY],\n","            bi_directional = config_defaults[IS_BIDIRECTIONAL_KEY]\n","            )\n","        \n","        attndecoder1 = DecoderRNNWithAttention(\n","            embedding_size = config_defaults[EMBEDDING_SIZE_KEY], \n","            hidden_size = config_defaults[HIDDEN_LAYER_KEY],\n","            num_layers_decoder = config_defaults[ENCODER_LAYER_KEY],\n","            cell_type = config_defaults[CELL_TYPE_KEY],\n","            drop_out = config_defaults[DROPOUT_KEY],\n","            # bi_directional = config_defaults[IS_BIDIRECTIONAL_KEY], \n","            max_length_word = max_len,\n","            output_size = output_langs.n_chars\n","            )\n","\n","        if is_gpu== True:\n","            encoder1 = encoder1.cuda()\n","            attndecoder1 = attndecoder1.cuda()\n","        print(\"with attention\")\n","        attention = True\n","        seq2seqWithAttention(\n","            encoder = encoder1,\n","            decoder = attndecoder1,\n","            train_loader = train_loader,\n","            val_loader = val_loader,\n","            test_loader = test_loader,\n","            learning_rate = config_defaults[LEARNING_RATE_KEY],\n","            optimizer = optimizer,\n","            epochs = config_defaults[EPOCHS_KEY],\n","            max_length_word = max_len,\n","            attention=attention,\n","            num_layers_enc = config_defaults[ENCODER_LAYER_KEY],\n","            output_lang = output_langs,\n","            input_lang = input_langs,\n","            batch_size = config_defaults[BATCH_SIZE_KEY],\n","            cell_type = config_defaults[CELL_TYPE_KEY],\n","            is_wandb = is_wandb\n","        )\n","        if is_heat_map:\n","            plot_heatmap(\n","                test_loader = test_loader,\n","                encoder = encoder1,\n","                decoder = attndecoder1,\n","                batch_size = config_defaults[BATCH_SIZE_KEY],\n","                num_layers_encoder = config_defaults[ENCODER_LAYER_KEY],\n","                cell_type = config_defaults[CELL_TYPE_KEY],\n","                max_length_word = max_len,\n","                input_lang = input_langs,\n","                output_lang = output_langs,\n","                bi_directional = config_defaults[IS_BIDIRECTIONAL_KEY]\n","                )\n","\n","    else:\n","        # Prepare training data\n","        input_langs,output_langs,pairs,max_len = prepareData(TRAIN_DATASET_PATH)\n","        print(\"train:sample:\", random.choice(pairs))\n","        train_n = len(pairs)\n","        print(f\"Number of training examples: {train_n}\")\n","\n","        # Prepare validation data\n","        input_langs,output_langs,val_pairs,max_len_val = prepareData(VALIDATION_DATASET_PATH)\n","        val_n = len(val_pairs)\n","        print(\"validation:sample:\", random.choice(val_pairs))\n","        print(f\"Number of validation examples: {val_n}\")\n","\n","        # Prepare test data\n","        input_langs,output_langs,test_pairs,max_len_test = prepareData(TEST_DATASET_PATH)\n","        test_n = len(test_pairs)\n","        print(\"Test:sample:\", random.choice(test_pairs))\n","        print(f\"Number of Test examples: {test_n}\")\n","\n","        max_len = max(max_len, max(max_len_val, max_len_test)) + 4\n","        print(max_len)\n","\n","        # Convert data to tensors and create data loaders\n","        pairs = makeTensor(input_langs, output_langs, pairs, max_len)\n","        val_pairs = makeTensor(input_langs, output_langs, val_pairs, max_len)\n","        test_pairs = makeTensor(input_langs, output_langs, test_pairs, max_len)\n","\n","        train_loader = DataLoader(dataset = pairs, batch_size=config_defaults[BATCH_SIZE_KEY], shuffle=True)\n","        val_loader = DataLoader(dataset = val_pairs, batch_size=config_defaults[BATCH_SIZE_KEY], shuffle=True)\n","        test_loader = DataLoader(dataset = test_pairs, batch_size=config_defaults[BATCH_SIZE_KEY], shuffle=True)\n","\n","        # Create the encoder and decoder models\n","        encoder1 = EncoderRNN(\n","            input_size = input_langs.n_chars,\n","            embedding_size =  config_defaults[EMBEDDING_SIZE_KEY],\n","            hidden_size =  config_defaults[HIDDEN_LAYER_KEY],\n","            num_layers_encoder = config_defaults[ENCODER_LAYER_KEY],\n","            cell_type = config_defaults[CELL_TYPE_KEY],\n","            drop_out = config_defaults[DROPOUT_KEY],\n","            bi_directional = config_defaults[IS_BIDIRECTIONAL_KEY]\n","            )\n","        decoder1 = DecoderRNN(\n","            embedding_size = config_defaults[EMBEDDING_SIZE_KEY], \n","            hidden_size = config_defaults[HIDDEN_LAYER_KEY],\n","            num_layers_decoder = config_defaults[ENCODER_LAYER_KEY],\n","            cell_type = config_defaults[CELL_TYPE_KEY],\n","            drop_out = config_defaults[DROPOUT_KEY],\n","            bi_directional = config_defaults[IS_BIDIRECTIONAL_KEY], \n","            output_size = output_langs.n_chars\n","            )\n","\n","        if is_gpu:\n","            encoder1, decoder1 = encoder1.cuda(), decoder1.cuda()\n","\n","        print(\"vanilla seq2seqWithoutAttn\")\n","        # Train and evaluate the Seq2SeqWithoutAttn model\n","        seq2seq(\n","            encoder = encoder1,\n","            decoder = decoder1,\n","            train_loader = train_loader,\n","            val_loader = val_loader,\n","            test_loader = test_loader,\n","            lr = config_defaults[LEARNING_RATE_KEY],\n","            optimizer = optimizer,\n","            epochs = config_defaults[EPOCHS_KEY],\n","            max_length_word = max_len,\n","            num_layers_enc = config_defaults[ENCODER_LAYER_KEY],\n","            output_lang = output_langs,\n","            input_langs = input_langs,\n","            batch_size = config_defaults[BATCH_SIZE_KEY],\n","            cell_type = config_defaults[CELL_TYPE_KEY],\n","            is_wandb = is_wandb\n","            )\n","        if is_heat_map:\n","            plot_heatmap(\n","                test_loader = test_loader,\n","                encoder = encoder1,\n","                decoder = decoder1,\n","                batch_size = config_defaults[BATCH_SIZE_KEY],\n","                num_layers_encoder = config_defaults[ENCODER_LAYER_KEY],\n","                cell_type = config_defaults[CELL_TYPE_KEY],\n","                max_length_word = max_len,\n","                input_lang = input_langs,\n","                output_lang = output_langs,\n","                bi_directional = config_defaults[IS_BIDIRECTIONAL_KEY]\n","                )\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_id = wandb.sweep(sweep_config, project=\"dl-assignment-3\", entity=\"cs23m007\")\n","print('sweep_id: ', sweep_id)\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4830499,"sourceId":8163901,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
