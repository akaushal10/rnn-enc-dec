{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:12:33.187567Z","iopub.status.busy":"2024-04-24T07:12:33.187191Z","iopub.status.idle":"2024-04-24T07:12:38.671156Z","shell.execute_reply":"2024-04-24T07:12:38.670194Z","shell.execute_reply.started":"2024-04-24T07:12:33.187539Z"},"trusted":true},"outputs":[],"source":["import os\n","import wandb\n","import torch\n","import torch.nn as nn\n","import random\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as Function\n","import argparse\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:14:11.800650Z","iopub.status.busy":"2024-04-24T07:14:11.799999Z","iopub.status.idle":"2024-04-24T07:14:11.828616Z","shell.execute_reply":"2024-04-24T07:14:11.827611Z","shell.execute_reply.started":"2024-04-24T07:14:11.800598Z"},"trusted":true},"outputs":[],"source":["SYMBOL_BEGIN, SYMBOL_END, SYMBOL_UNKNOWN, SYMBOL_PADDING = 0, 1, 2, 3\n","\n","INPUT_LABEL = \"input\"\n","TARGET_LABEL = \"target\"\n","DELIMETER = \",\"\n","\n","RNN_KEY = \"RNN\"\n","GRU_KEY = \"GRU\"\n","LSTM_KEY = \"LSTM\"\n","\n","INPUT_LANG_KEY = \"input_lang\"\n","OUTPUT_LANG_KEY = \"output_lang\"\n","PAIRS_KEY = \"pairs\"\n","MAX_LEN_KEY = \"max_len\"\n","\n","input_lang = \"eng\"\n","TARGET_LANG = \"hin\"\n","\n","TRAIN_LABEL = \"train\"\n","TEST_LABEL = \"test\"\n","VALID_LABEL = \"valid\"\n","\n","DEFAULT_PATH = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled\"\n","TRAIN_DATASET_PATH = f\"{DEFAULT_PATH}/{TARGET_LANG}/{TARGET_LANG}_{TRAIN_LABEL}.csv\"\n","VALIDATION_DATASET_PATH = f\"{DEFAULT_PATH}/{TARGET_LANG}/{TARGET_LANG}_{VALID_LABEL}.csv\"\n","TEST_DATASET_PATH = f\"{DEFAULT_PATH}/{TARGET_LANG}/{TARGET_LANG}_{TEST_LABEL}.csv\"\n","\n","NADAM_KEY = \"Nadam\"\n","\n","# Sweep param labels\n","EMBEDDING_SIZE_KEY = \"embedding_size\"\n","EPOCHS_KEY = \"epochs\"\n","ENCODER_LAYER_KEY = \"encoder_layers\"\n","DECODER_LAYER_KEY = \"decoder_layers\"\n","HIDDEN_LAYER_KEY = \"hidden_layer\"\n","IS_BIDIRECTIONAL_KEY = \"bidirectional\"\n","DROPOUT_KEY = \"dropout\"\n","CELL_TYPE_KEY = \"cell_type\"\n","LEARNING_RATE_KEY = \"learning_rate\"\n","BATCH_SIZE_KEY = \"batch_size\"\n","\n","# wandb constants\n","WANDB_PROJECT_NAME=\"dl-assignment-3\"\n","WANDB_ENTITY_NAME=\"cs23m007\"\n","\n","# wandb plot titles\n","TRAIN_ACCURACY_TITLE = \"train_acc\"\n","VALIDATION_ACCURACY_TITLE = \"val_acc\"\n","TEST_ACCURACY_TITLE = \"test_acc\"\n","TRAIN_LOSS_TITLE = \"train_loss\"\n","VALIDATION_LOSS_TITLE = \"val_loss\"\n","TEST_LOSS_TITLE = \"test_loss\"\n","\n","best_params = {\n","    EMBEDDING_SIZE_KEY :256,\n","    EPOCHS_KEY :5,\n","    ENCODER_LAYER_KEY :2,\n","    DECODER_LAYER_KEY :2,\n","    HIDDEN_LAYER_KEY :256,\n","    IS_BIDIRECTIONAL_KEY :False,\n","    DROPOUT_KEY :0.2,\n","    CELL_TYPE_KEY :LSTM_KEY,\n","    BATCH_SIZE_KEY : 32,\n","    LEARNING_RATE_KEY: 0.001\n","\n","}\n","\n","\n","\n","\n","# Set the device type to CUDA if available, otherwise use CPU\n","is_gpu = torch.cuda.is_available()\n","if is_gpu:\n","    device = torch.device(\"cuda\")\n","else:\n","    device = torch.device(\"cpu\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_config = {\n","    \"name\" : \"CS6910_Assignemnt3_Without Attention\",\n","    \"method\" : \"random\",\n","    'metric': {\n","        'name': VALIDATION_ACCURACY_TITLE,\n","        'goal': 'maximize'\n","    },\n","    \"parameters\" : {\n","        EMBEDDING_SIZE_KEY : {\n","          \"values\" : [16, 32, 64, 256]  \n","        },\n","        EPOCHS_KEY : {\n","            \"values\" : [5,10]\n","        },\n","        ENCODER_LAYER_KEY: {\n","            \"values\": [1,2,3]\n","        },\n","        DECODER_LAYER_KEY: {\n","            \"values\": [1,2,3]\n","        },\n","        HIDDEN_LAYER_KEY:{\n","            \"values\": [16, 32, 64, 256]\n","        },\n","        IS_BIDIRECTIONAL_KEY:{\n","            \"values\": [True, False]\n","        },\n","        DROPOUT_KEY: {\n","            \"values\": [0,0.2,0.3]       \n","        }, \n","        CELL_TYPE_KEY: {\n","            \"values\": [RNN_KEY,GRU_KEY,LSTM_KEY]       \n","        },\n","        LEARNING_RATE_KEY:{\n","            \"values\":[0.001,0.01]\n","        },\n","        BATCH_SIZE_KEY:{\n","            \"values\":[32,64,128] \n","        }\n","    }\n","}"]},{"cell_type":"markdown","metadata":{},"source":["# Utility Functions and classes"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:14:13.654369Z","iopub.status.busy":"2024-04-24T07:14:13.653678Z","iopub.status.idle":"2024-04-24T07:14:13.668777Z","shell.execute_reply":"2024-04-24T07:14:13.667773Z","shell.execute_reply.started":"2024-04-24T07:14:13.654332Z"},"trusted":true},"outputs":[],"source":["class Vocabulary:\n","    def __init__(self):\n","        self.str_count,self.int_encodding = dict(),dict()\n","        self.n_chars = 4\n","        self.str_encodding = {0: \"<\", 1: \">\", 2: \"?\", 3: \".\"}\n","\n","    def addWord(self, word):\n","        for char in word:\n","            try:\n","                self.str_count[char] += 1\n","            except:\n","                self.int_encodding[char] = self.n_chars\n","                self.str_encodding[self.n_chars] = char\n","                self.str_count[char] = 1\n","                self.n_chars += 1\n","\n","# prepareDataWithoutAttn\n","def prepareData(dir):\n","    data = pd.read_csv(dir, sep=DELIMETER, names=[INPUT_LABEL, TARGET_LABEL])\n","\n","    max_input_length = data[INPUT_LABEL].apply(len).max()\n","    max_target_length = data[TARGET_LABEL].apply(len).max()\n","    \n","    max_len=max(max_input_length,max_target_length)\n","\n","    input_lang, output_lang = Vocabulary(), Vocabulary()\n","\n","    pairs = pd.concat([data[INPUT_LABEL], data[TARGET_LABEL]], axis=1).values.tolist()\n","\n","    for pair in pairs:\n","        input_lang.addWord(pair[0])\n","        output_lang.addWord(pair[1])\n","\n","    return input_lang,output_lang,pairs,max_len\n","\n","\n","\n","def prepareDataWithAttention(dir):\n","\n","    data = pd.read_csv(dir, sep=DELIMETER, names=[INPUT_LABEL, TARGET_LABEL])\n","\n","    max_input_length = data[INPUT_LABEL].apply(len).max()\n","    max_target_length = data[TARGET_LABEL].apply(len).max()\n","    \n","\n","    input_lang, output_lang = Vocabulary(), Vocabulary()\n","    \n","\n","    pairs = pd.concat([data[INPUT_LABEL], data[TARGET_LABEL]], axis=1).values.tolist()\n","\n","    for pair in pairs:\n","        input_lang.addWord(pair[0])\n","        output_lang.addWord(pair[1])\n","\n","    prepared_data = {\n","        \"input_lang\": input_lang,\n","        \"output_lang\": output_lang,\n","        \"pairs\": pairs,\n","        \"max_input_length\": max_input_length,\n","        \"max_target_length\": max_target_length,\n","    }\n","\n","    return prepared_data\n","\n","\n","\n","# helpTensorWithoutAttn\n","def helpTensor(lang, word, max_length):\n","    index_list = []\n","    for char in word:\n","        try:\n","            index_list.append(lang.char2index[char])\n","        except:\n","            index_list.append(SYMBOL_UNKNOWN)\n","\n","    indexes = index_list\n","    indexes.append(SYMBOL_END)\n","    n = len(indexes)\n","    indexes.extend([SYMBOL_PADDING] * (max_length - n))\n","    result = torch.LongTensor(indexes)\n","    if is_gpu:\n","        return result.cuda()\n","    return result\n","\n","\n","def helpTensorWithAttention(lang, word, max_length):\n","    index_list=[]\n","    for char in word:\n","        try:\n","            index_list.append(lang.char2index[char])\n","        except:\n","            index_list.append(SYMBOL_UNKNOWN)\n","    indexes = index_list\n","    indexes.append(SYMBOL_END)\n","    n = len(indexes)\n","    indexes.extend([SYMBOL_PADDING] * (max_length - n))\n","    result = torch.LongTensor(indexes)\n","    if is_gpu:\n","        return result.cuda()\n","    return result\n","\n","\n","# MakeTensorWithoutAttn\n","def makeTensor(input_lang, output_lang, pairs, reach):\n","    res = [(helpTensor(input_lang, pairs[i][0], reach), helpTensor(output_lang, pairs[i][1], reach)) for i in range(len(pairs))]\n","    return res\n","\n","# accuracyWithoutAttn\n","def accuracy(encoder, decoder, loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang):\n","    with torch.no_grad():\n","        total = correct = 0\n","\n","        for batch_x, batch_y in loader:\n","            encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n","\n","            input_variable = Variable(batch_x.transpose(0, 1))\n","            target_variable = Variable(batch_y.transpose(0, 1))\n","\n","            if cell_type == LSTM_KEY:\n","                encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n","                encoder_hidden = (encoder_hidden, encoder_cell_state)\n","\n","\n","            output = torch.LongTensor(target_variable.size()[0], batch_size)\n","\n","            for ei in range(input_variable.size()[0]):\n","                encoder_hidden = encoder(input_variable[ei], batch_size, encoder_hidden)[1]\n","\n","            decoder_input = Variable(torch.LongTensor([SYMBOL_BEGIN] * batch_size))\n","            if is_gpu:\n","                decoder_input = decoder_input.cuda()\n","\n","            decoder_hidden = encoder_hidden\n","\n","            # Decoder forward pass\n","            for di in range(target_variable.size()[0]):\n","                decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n","                topi = decoder_output.data.topk(1)[1]\n","                output[di], decoder_input = torch.cat(tuple(topi)), torch.cat(tuple(topi))\n","            output = output.transpose(0, 1)\n","\n","            # Calculate accuracyWithoutAttn\n","            for di in range(output.size()[0]):\n","                ignore = [SYMBOL_BEGIN, SYMBOL_END, SYMBOL_PADDING]\n","                sent = [output_lang.str_encodding[letter.item()] for letter in output[di] if letter not in ignore]\n","                y = [output_lang.str_encodding[letter.item()] for letter in batch_y[di] if letter not in ignore]\n","                if sent == y:\n","                    correct += 1\n","                total += 1\n","\n","    return (correct / total) * 100\n","\n","def accuracyWithAttention(\n","    encoder,\n","    decoder,\n","    loader,\n","    batch_size,\n","    num_layers_enc,\n","    cell_type,\n","    output_lang,\n","    criterion,\n","    max_length,\n","    ):\n","\n","    with torch.no_grad():\n","\n","        total = correct = 0\n","\n","        for batch_x, batch_y in loader:\n","\n","            encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n","\n","            input_variable = Variable(batch_x.transpose(0, 1))\n","            target_variable = Variable(batch_y.transpose(0, 1))\n","\n","            if cell_type == LSTM_KEY:\n","                encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n","                encoder_hidden = (encoder_hidden, encoder_cell_state)\n","\n","\n","            output = torch.LongTensor(target_variable.size()[0], batch_size)\n","\n","            encoder_outputs = Variable(\n","                torch.zeros(max_length, batch_size, encoder.hidden_size)\n","            )\n","            if is_gpu: \n","                encoder_outputs = encoder_outputs.cuda() \n","\n","            for ei in range(input_variable.size()[0]):\n","                encoder_output, encoder_hidden = encoder(\n","                    input_variable[ei], batch_size, encoder_hidden\n","                )\n","                encoder_outputs[ei] = encoder_output[0]\n","\n","            decoder_input = Variable(torch.LongTensor([SYMBOL_BEGIN] * batch_size))\n","            if is_gpu:\n","                decoder_input = decoder_input.cuda() \n","\n","            decoder_hidden = encoder_hidden\n","\n","            for di in range(target_variable.size()[0]):\n","                decoder_output, decoder_hidden, decoder_attention = decoder(\n","                    decoder_input,\n","                    batch_size,\n","                    decoder_hidden,\n","                    encoder_outputs.reshape(\n","                        batch_size, max_length, encoder.hidden_size\n","                    ),\n","                )\n","                topi = decoder_output.data.topk(1)[1]\n","                decoder_input,output[di] = torch.cat(tuple(topi)),torch.cat(tuple(topi))\n","            output = output.transpose(0, 1)\n","\n","            for di in range(output.size()[0]):\n","                ignore = [SYMBOL_BEGIN, SYMBOL_END, SYMBOL_PADDING]\n","                sent = [output_lang.str_encodding[letter.item()] for letter in output[di] if letter not in ignore]\n","                y = [output_lang.str_encodding[letter.item()] for letter in batch_y[di] if letter not in ignore]\n","                if sent == y:\n","                    correct += 1\n","                total += 1\n","\n","    return (correct / total) * 100\n","\n","\n","# calc_lossWithoutAttn\n","def calc_loss(encoder, decoder, input_tensor, target_tensor, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training, teacher_forcing_ratio=0.5):\n","    # Initialize the encoder hidden state\n","    output_hidden = encoder.initHidden(batch_size, num_layers_enc)\n","\n","    # Check if LSTM and initialize cell state\n","    if cell_type == LSTM_KEY:\n","        encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n","        output_hidden = (output_hidden, encoder_cell_state)\n","\n","    # Zero the gradients\n","    encoder_optimizer.zero_grad()\n","    decoder_optimizer.zero_grad()\n","\n","    # Get input and target sequence lengths\n","    # input_length = input_tensor.size(0)\n","    # target_length = target_tensor.size(0)\n","\n","    # Initialize loss\n","    loss = 0\n","\n","    # Encoder forward pass\n","    for ei in range(input_tensor.size(0)):\n","        output_hidden = encoder(input_tensor[ei], batch_size, output_hidden)[1]\n","\n","    # Initialize decoder input\n","    decoder_input = torch.LongTensor([SYMBOL_BEGIN] * batch_size)\n","    decoder_input = decoder_input.cuda() if is_gpu else decoder_input\n","\n","    # Determine if using teacher forcing\n","    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n","\n","    # Loop over target sequence\n","    if is_training:\n","        # Training phase\n","        for di in range(target_tensor.size(0)):\n","            decoder_output, output_hidden = decoder(decoder_input, batch_size, output_hidden)\n","            decoder_input = target_tensor[di] if use_teacher_forcing else decoder_output.argmax(dim=1)\n","            loss = criterion(decoder_output, target_tensor[di]) + loss\n","    else:\n","        # Validation phase\n","        with torch.no_grad():\n","            for di in range(target_tensor.size(0)):\n","                decoder_output, output_hidden = decoder(decoder_input, batch_size, output_hidden)\n","                loss += criterion(decoder_output, target_tensor[di])\n","                decoder_input = decoder_output.argmax(dim=1)\n","\n","    # Backpropagation and optimization in training phase\n","    if is_training:\n","        loss.backward()\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","    # Return the average loss per target length\n","    return loss.item() / target_tensor.size(0)\n","\n","def calcLossWithAttention(\n","    encoder,\n","    decoder,\n","    encoder_optimizer,\n","    decoder_optimizer,\n","    input_tensor,\n","    target_tensor,\n","    criterion,\n","    batch_size,\n","    cell_type,\n","    num_layers_enc,\n","    max_length,is_training,\n","    teacher_forcing_ratio=0.5,\n","):\n","\n","    output_hidden = encoder.initHidden(batch_size, num_layers_enc)\n","\n","    if cell_type == LSTM_KEY:\n","        encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n","        output_hidden = (output_hidden, encoder_cell_state)\n","\n","    if is_training:\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","\n","    input_length = input_tensor.size(0)\n","    target_length = target_tensor.size(0)\n","\n","    encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size))\n","    if is_gpu:\n","        encoder_outputs = encoder_outputs.cuda()\n","    loss = 0\n","\n","    for ei in range(input_length):\n","        encoder_output, output_hidden = encoder(\n","            input_tensor[ei], batch_size, output_hidden\n","        )\n","        encoder_outputs[ei] = encoder_output[0]\n","\n","    decoder_input = Variable(torch.LongTensor([SYMBOL_BEGIN] * batch_size))\n","    if is_gpu :\n","        decoder_input = decoder_input.cuda() \n","    decoder_hidden = output_hidden\n","    if is_training:\n","        use_teacher_forcing = False\n","        if random.random() < teacher_forcing_ratio:\n","            use_teacher_forcing = True\n","        n = target_length\n","        if not use_teacher_forcing :\n","            for di in range(n):\n","                decoder_output, decoder_hidden, decoder_attention = decoder(\n","                    decoder_input,\n","                    batch_size,\n","                    decoder_hidden,\n","                    encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n","                )\n","                #2 for loop ko bhar dal de\n","                topv, topi = decoder_output.data.topk(1)\n","                decoder_input = torch.cat(tuple(topi))\n","                if is_gpu :\n","                    decoder_input = decoder_input.cuda() \n","                loss += criterion(decoder_output, target_tensor[di])\n","        else:\n","            for di in range(n):\n","                decoder_output, decoder_hidden, decoder_attention = decoder(\n","                    decoder_input,\n","                    batch_size,\n","                    decoder_hidden,\n","                    encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n","                )\n","                loss += criterion(decoder_output, target_tensor[di])\n","                decoder_input = target_tensor[di]\n","            \n","\n","        loss.backward()\n","\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","    else :\n","        for di in range(target_length):\n","            decoder_output, decoder_hidden, decoder_attention = decoder(\n","                decoder_input,\n","                batch_size,\n","                decoder_hidden,\n","                encoder_outputs.reshape(batch_size, max_length, encoder.hidden_size),\n","            )\n","            topv, topi = decoder_output.data.topk(1)\n","            decoder_input = torch.cat(tuple(topi))\n","\n","            if is_gpu:\n","                decoder_input = decoder_input.cuda()\n","            loss += criterion(decoder_output, target_tensor[di])\n","\n","    avg_loss = loss.item() / target_length\n","    return avg_loss\n","\n","\n","\n","\n","# Train and evaluate the Seq2SeqWithoutAttn model\n","def seq2seq(encoder, decoder, train_loader, val_loader, test_loader, lr, optimizer, epochs, max_length_word, num_layers_enc, output_lang,batch_size,cell_type,is_wandb):\n","    max_length = max_length_word - 1\n","    # Define the optimizer and criterion\n","    encoder_optimizer = optim.NAdam(encoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(encoder.parameters(), lr=lr)\n","    decoder_optimizer = optim.NAdam(decoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(decoder.parameters(), lr=lr)\n","    criterion = nn.NLLLoss()\n","\n","    for epoch in range(epochs):\n","        train_loss_total = 0\n","        val_loss_total = 0\n","\n","        # Training phase\n","        for batch_x, batch_y in train_loader:\n","            batch_x = Variable(batch_x.transpose(0, 1))\n","            batch_y = Variable(batch_y.transpose(0, 1))\n","            # Calculate the training loss\n","            loss = calc_loss(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=True)\n","            train_loss_total += loss\n","\n","        train_loss_avg = train_loss_total / len(train_loader)\n","        print(f\"Epoch: {epoch} | Train Loss: {train_loss_avg:.4f} |\", end=\"\")\n","\n","        # Validation phase\n","        for batch_x, batch_y in val_loader:\n","            batch_x = Variable(batch_x.transpose(0, 1))\n","            batch_y = Variable(batch_y.transpose(0, 1))\n","            # Calculate the validation loss\n","            loss = calc_loss(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=False)\n","            val_loss_total += loss\n","\n","        val_loss_avg = val_loss_total / len(val_loader)\n","        print(f\"Val Loss: {val_loss_avg:.4f} |\", end=\"\")\n","\n","\n","        train_acc = accuracy(encoder, decoder, train_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n","        train_acc /= 100\n","        print(f\"train Accuracy: {train_acc:.4%} |\", end=\"\")\n","\n","        # Calculate validation accuracyWithoutAttn\n","        val_acc = accuracy(encoder, decoder, val_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n","        val_acc /= 100\n","        print(f\"Val Accuracy: {val_acc:.4%} |\", end=\"\")\n","        \n","        test_acc = accuracy(encoder, decoder, test_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n","        test_acc /= 100\n","        print(f\"Test Accuracy: {test_acc:.4%}\")\n","        if is_wandb:\n","            wandb.log(\n","                {\n","                    TRAIN_ACCURACY_TITLE: train_acc,\n","                    VALIDATION_ACCURACY_TITLE: val_acc,\n","                    TEST_ACCURACY_TITLE: test_acc,\n","                    TRAIN_LOSS_TITLE: train_loss_avg,\n","                    VALIDATION_LOSS_TITLE: val_loss_avg,\n","                    # TEST_LOSS_TITLE: test_loss\n","                }\n","            )\n","\n","\n","def seq2seqWithAttention(\n","    encoder,\n","    decoder,\n","    train_loader,\n","    val_loader,\n","    test_loader,\n","    learning_rate,\n","    optimizer,\n","    epochs,\n","    max_length_word,\n","    attention,\n","    num_layers_enc,\n","    output_lang,\n","    batch_size,\n","    cell_type,\n","    is_wandb\n"," ):\n","    max_length = max_length_word - 1\n","    n_val = len(val_loader)\n","    n_train = len(train_loader)\n","    encoder_optimizer = (\n","        optim.NAdam(encoder.parameters(), lr=learning_rate)\n","        if optimizer == \"nadam\"\n","        else optim.Adam(encoder.parameters(), lr=learning_rate)\n","    )\n","    decoder_optimizer = (\n","        optim.NAdam(decoder.parameters(), lr=learning_rate)\n","        if optimizer == \"nadam\"\n","        else optim.Adam(decoder.parameters(), lr=learning_rate)\n","    )\n","    criterion = nn.NLLLoss()\n","\n","    for epoch in range(epochs):\n","        train_loss_total, val_loss_total = 0, 0\n","        \n","        for batchx, batchy in train_loader:\n","            batchx = Variable(batchx.transpose(0, 1))\n","            batchy = Variable(batchy.transpose(0, 1))\n","            loss = calcLossWithAttention(\n","                encoder = encoder,\n","                decoder =decoder,\n","                encoder_optimizer = encoder_optimizer,\n","                decoder_optimizer = decoder_optimizer,\n","                input_tensor = batchx,\n","                target_tensor = batchy,\n","                criterion = criterion,\n","                batch_size =  batch_size,\n","                cell_type = cell_type,\n","                num_layers_enc = num_layers_enc,\n","                max_length = max_length + 1,\n","                is_training = True, #is_training\n","                teacher_forcing_ratio=0.5\n","            )\n","            train_loss_total = train_loss_total + loss\n","        train_loss_avg = train_loss_total / n_train\n","        print(f\"Epoch: {epoch} | Train Loss: {train_loss_avg:.4f} | \", end=\"\")\n","\n","        for batchx, batchy in val_loader:\n","            batchx = Variable(batchx.transpose(0, 1))\n","            batchy = Variable(batchy.transpose(0, 1))\n","            loss = calcLossWithAttention(\n","                encoder = encoder,\n","                decoder = decoder,\n","                encoder_optimizer = encoder_optimizer,\n","                decoder_optimizer = decoder_optimizer,\n","                input_tensor = batchx,\n","                target_tensor = batchy,\n","                criterion = criterion,\n","                batch_size =  batch_size,\n","                cell_type = cell_type,\n","                num_layers_enc = num_layers_enc,\n","                max_length = max_length + 1,\n","                is_training = False, #is_training\n","                teacher_forcing_ratio=0.5\n","            )\n","\n","            val_loss_total = val_loss_total+ loss\n","\n","        val_loss_avg = val_loss_total / n_val\n","        print(f\"Val Loss: {val_loss_avg:.4f} | \", end=\"\")\n","\n","        train_acc = accuracyWithAttention(\n","            encoder = encoder,\n","            decoder = decoder,\n","            loader = train_loader,\n","            batch_size = batch_size,\n","            num_layers_enc = num_layers_enc,\n","            cell_type = cell_type,\n","            output_lang = output_lang,\n","            criterion = criterion,\n","            max_length = max_length,\n","        ) \n","        train_acc = train_acc /  100\n","        print(f\"Train Accuracy: {train_acc:.4%} |\", end=\"\")\n","\n","        val_acc = accuracyWithAttention(\n","            encoder = encoder,\n","            decoder = decoder,\n","            loader = val_loader,\n","            batch_size = batch_size,\n","            num_layers_enc = num_layers_enc,\n","            cell_type = cell_type,\n","            output_lang = output_lang,\n","            criterion = criterion,\n","            max_length = max_length + 1,\n","        )\n","        val_acc = val_acc / 100\n","        print(f\"Val Accuracy: {val_acc:.4%} |\", end=\"\")\n","        test_acc = accuracyWithAttention(\n","            encoder = encoder,\n","            decoder = decoder,\n","            loader = test_loader,\n","            batch_size = batch_size,\n","            num_layers_enc = num_layers_enc,\n","            cell_type = cell_type,\n","            output_lang = output_lang,\n","            criterion = criterion,\n","            max_length = max_length + 1,\n","        )\n","\n","        test_acc = test_acc / 100\n","        print(f\"Test Accuracy: {test_acc:.4%}\")\n","\n","        if is_wandb:\n","            wandb.log(\n","                {\n","                    TRAIN_ACCURACY_TITLE: train_acc,\n","                    VALIDATION_ACCURACY_TITLE: val_acc,\n","                    TEST_ACCURACY_TITLE: test_acc,\n","                    TRAIN_LOSS_TITLE: train_loss_avg,\n","                    VALIDATION_LOSS_TITLE: val_loss_avg,\n","                    # TEST_LOSS_TITLE: test_loss\n","                }\n","            )\n","\n","\n","\n"," \n"]},{"cell_type":"markdown","metadata":{},"source":["# Encoder Class"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-24T07:20:36.782415Z","iopub.status.busy":"2024-04-24T07:20:36.782044Z","iopub.status.idle":"2024-04-24T07:20:36.837554Z","shell.execute_reply":"2024-04-24T07:20:36.836502Z","shell.execute_reply.started":"2024-04-24T07:20:36.782386Z"},"trusted":true},"outputs":[],"source":["# EncoderRNNWithoutAttn\n","class EncoderRNN(nn.Module):\n","    def __init__(self, input_size, embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional):\n","        super(EncoderRNN, self).__init__()\n","\n","        self.emb_n = embedding_size\n","        self.hid_n = hidden_size\n","        self.encoder_n = num_layers_encoder\n","        self.model_key = cell_type\n","        self.is_dropout = drop_out\n","        self.is_bi_dir = bi_directional\n","\n","        self.embedding = nn.Embedding(input_size, self.emb_n)\n","        self.dropout = nn.Dropout(self.is_dropout)\n","\n","        cell_map = dict({RNN_KEY: nn.RNN, GRU_KEY: nn.GRU, LSTM_KEY: nn.LSTM})\n","        self.cell_layer = cell_map[self.model_key](\n","            input_size = self.emb_n,\n","            hidden_size = self.hid_n,\n","            num_layers=self.encoder_n,\n","            dropout=self.is_dropout,\n","            bidirectional=self.is_bi_dir,\n","        )\n","\n","    def forward(self, input, batch_size, hidden):\n","        pre_embedded = self.embedding(input)\n","        transformed_embedded_data = pre_embedded.view(1, batch_size, -1)\n","        embedded = self.dropout(transformed_embedded_data)\n","        y_cap, hidden = self.cell_layer(embedded, hidden)\n","        return y_cap, hidden\n","\n","    def initHidden(self, batch_size, num_layers_enc):\n","        if self.is_bi_dir:\n","            weights = torch.zeros(num_layers_enc * 2 , batch_size, self.hid_n)\n","        else:\n","            weights = torch.zeros(num_layers_enc, batch_size, self.hid_n)\n","\n","        if is_gpu:\n","            return weights.cuda()\n","        return weights\n","    \n","\n","class EncoderRNNWithAttention(nn.Module):\n","    def __init__(self, input_size, embedding_size,hidden_size,num_layers_encoder,cell_type,drop_out,bi_directional):\n","        super(EncoderRNNWithAttention, self).__init__()\n","\n","        self.emb_n = embedding_size\n","        self.hid_n = hidden_size\n","        self.encoder_n = num_layers_encoder\n","        self.model_key = cell_type\n","        self.is_dropout = drop_out\n","        self.is_bi_dir = bi_directional\n","\n","        self.embedding = nn.Embedding(input_size, self.emb_n)\n","        self.dropout = nn.Dropout(self.is_dropout)\n","\n","        cell_map = dict({RNN_KEY: nn.RNN, GRU_KEY: nn.GRU, LSTM_KEY: nn.LSTM})\n","        self.cell_layer = cell_map[self.model_key](\n","            input_size = self.emb_n,\n","            hidden_size = self.hid_n,\n","            num_layers=self.encoder_n,\n","            dropout=self.is_dropout,\n","            bidirectional=self.is_bi_dir,\n","        )\n","\n","    def forward(self, input, batch_size, hidden):\n","        pre_embedded = self.embedding(input)\n","        transformed_embedded_data = pre_embedded.view(1, batch_size, -1)\n","        embedded = self.dropout(transformed_embedded_data)\n","        y_cap, hidden = self.cell_layer(embedded, hidden)\n","        return y_cap, hidden\n","\n","    def initHidden(self, batch_size, num_layers_enc):\n","        if self.is_bi_dir:\n","            weights = torch.zeros(num_layers_enc * 2 , batch_size, self.hid_n)\n","        else:\n","            weights = torch.zeros(num_layers_enc, batch_size, self.hid_n)\n","\n","        if is_gpu:\n","            return weights.cuda()\n","        return weights\n"," \n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Decoder Class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# DecoderRNNWithoutAttn\n","class DecoderRNN(nn.Module):\n","    def __init__(self, embedding_size, hidden_size, num_layers_decoder, cell_type, drop_out, bi_directional, output_size):\n","        super(DecoderRNN, self).__init__()\n","\n","        self.emb_n = embedding_size\n","        self.hid_n = hidden_size\n","        self.decoder_n = num_layers_decoder\n","        self.model_key = cell_type\n","        self.is_dropout = drop_out\n","        self.is_bi_dir = bi_directional\n","\n","        # Create an embedding layer\n","        self.embedding = nn.Embedding(output_size, self.emb_n)\n","        self.dropout = nn.Dropout(self.is_dropout)\n","\n","        cell_map = dict({RNN_KEY: nn.RNN, GRU_KEY: nn.GRU, LSTM_KEY: nn.LSTM})\n","\n","        if self.cell_type in cell_map:\n","            self.cell_layer = cell_map[self.model_key](\n","                input_size = self.emb_n,\n","                hidden_size = self.hid_n,\n","                num_layers=self.decoder_n,\n","                dropout=self.is_dropout,\n","                bidirectional=self.is_bi_dir,\n","            )\n","\n","        # Linear layer for output\n","        if self.is_bi_dir :\n","            self.out = nn.Linear(self.hid_n * 2, output_size)\n","        else:\n","            self.out = nn.Linear(self.hid_n,output_size)\n","\n","        # Softmax activation\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    def forward(self, input, batch_size, hidden):\n","        y_cap = Function.relu(self.dropout(self.embedding(input).view(1, batch_size, -1)))\n","        y_cap, hidden = self.cell_layer(y_cap, hidden)\n","\n","        y_cap = self.softmax(self.out(y_cap[0]))\n","        return y_cap, hidden\n","\n","\n","class DecoderRNNWithAttention(nn.Module):\n","    def __init__(\n","        self,\n","        hidden_size,\n","        embedding_size,\n","        cell_type,\n","        num_layers_decoder,\n","        drop_out,\n","        max_length_word,\n","        output_size,\n","    ):\n","\n","        super(DecoderRNNWithAttention, self).__init__()\n","\n","        self.hid_n = hidden_size\n","        self.emb_n = embedding_size\n","        self.model_key = cell_type\n","        self.decoder_n = num_layers_decoder\n","        self.drop_out = drop_out\n","        self.max_length_word = max_length_word\n","\n","        self.embedding = nn.Embedding(output_size, embedding_dim=self.emb_n)\n","        layer_size = self.emb_n + self.hid_n \n","        self.attention_layer = nn.Linear(\n","            layer_size , self.max_length_word\n","        )\n","        self.attention_combine = nn.Linear(\n","            layer_size, self.emb_n\n","        )\n","        self.dropout = nn.Dropout(self.drop_out)\n","\n","        self.cell_layer = None\n","        cell_map = dict({RNN_KEY: nn.RNN, GRU_KEY: nn.GRU, LSTM_KEY: nn.LSTM})\n","\n","        if self.model_key in cell_map:\n","            self.cell_layer = cell_map[self.model_key](\n","                self.emb_n,\n","                self.hid_n,\n","                num_layers=self.decoder_n,\n","                dropout=self.drop_out,\n","            )\n","\n","        self.out = nn.Linear(self.hid_n, output_size)\n","\n","    def forward(self, input, batch_size, hidden, encoder_outputs):\n","        pre_embedded = self.embedding(input)\n","        embedded = pre_embedded.view(1, batch_size, -1)\n","\n","        attention_weights = None\n","\n","        attention_weights = Function.softmax(\n","            self.attention_layer(torch.cat((embedded[0],hidden[0][0] if self.model_key != LSTM_KEY else  hidden[0]), 1)), dim=1\n","        )\n","\n","        attention_applied = torch.bmm(\n","            attention_weights.view(batch_size, 1, self.max_length_word),\n","            encoder_outputs,\n","        ).view(1, batch_size, -1)\n","\n","        y_cap = torch.cat((embedded[0], attention_applied[0]), 1)\n","        y_cap = Function.relu(self.attention_combine(y_cap).unsqueeze(0))\n","        # if self.cell_type=RNN\" :\n","        y_cap, hidden = self.cell_layer(y_cap, hidden)\n","        y_cap = Function.log_softmax(self.out(y_cap[0]), dim=1)\n","\n","        return y_cap, hidden, attention_weights\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train Function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train(config_defaults = best_params,flag = False,is_wandb = True):\n","    optimizer = NADAM_KEY\n","    if flag:\n","        pass\n","    else:\n","        # Prepare training data\n","        if is_wandb:\n","            wandb.init(project=WANDB_PROJECT_NAME, entity=WANDB_ENTITY_NAME,config = config_defaults)\n","            args = wandb.config\n","            # Set the name of the run\n","\n","            wandb.run.name = 'ep-'+str(args[EPOCHS_KEY])+'-lr-'+str(args[LEARNING_RATE_KEY])+'-bs-'+str(args[BATCH_SIZE_KEY])+'-el-'+str(args[ENCODER_LAYER_KEY])+'-dl-'+str(args[DECODER_LAYER_KEY]) \\\n","                            +'-hl-'+str(args[HIDDEN_LAYER_KEY])+'-do-'+ str(args[DROPOUT_KEY])+ '-es-'+str(args[EMBEDDING_SIZE_KEY]) \\\n","                            + '-is_bd-'+str(args[IS_BIDIRECTIONAL_KEY])+'-model'+str(args[CELL_TYPE_KEY])\n","\n","        input_langs,output_langs,pairs,max_len = prepareData(TRAIN_DATASET_PATH)\n","        print(\"train:sample:\", random.choice(pairs))\n","        train_n = len(pairs)\n","        print(f\"Number of training examples: {train_n}\")\n","\n","        # Prepare validation data\n","        input_langs,output_langs,val_pairs,max_len_val = prepareData(VALIDATION_DATASET_PATH)\n","        val_n = len(val_pairs)\n","        print(\"validation:sample:\", random.choice(val_pairs))\n","        print(f\"Number of validation examples: {val_n}\")\n","\n","        # Prepare test data\n","        input_langs,output_langs,test_pairs,max_len_test = prepareData(TEST_DATASET_PATH)\n","        test_n = len(test_pairs)\n","        print(\"Test:sample:\", random.choice(test_pairs))\n","        print(f\"Number of Test examples: {test_n}\")\n","\n","        max_len = max(max_len, max(max_len_val, max_len_test)) + 4\n","        print(max_len)\n","\n","        # Convert data to tensors and create data loaders\n","        pairs = makeTensor(input_langs, output_langs, pairs, max_len)\n","        val_pairs = makeTensor(input_langs, output_langs, val_pairs, max_len)\n","        test_pairs = makeTensor(input_langs, output_langs, test_pairs, max_len)\n","\n","        train_loader = DataLoader(dataset = pairs, batch_size=config_defaults[BATCH_SIZE_KEY], shuffle=True)\n","        val_loader = DataLoader(dataset = val_pairs, batch_size=config_defaults[BATCH_SIZE_KEY], shuffle=True)\n","        test_loader = DataLoader(dataset = test_pairs, batch_size=config_defaults[BATCH_SIZE_KEY], shuffle=True)\n","\n","        # Create the encoder and decoder models\n","        encoder1 = EncoderRNN(\n","            input_size = input_langs.n_chars,\n","            embedding_size =  config_defaults[EMBEDDING_SIZE_KEY],\n","            hidden_size =  config_defaults[HIDDEN_LAYER_KEY],\n","            num_layers_encoder = config_defaults[ENCODER_LAYER_KEY],\n","            cell_type = config_defaults[CELL_TYPE_KEY],\n","            drop_out = config_defaults[DROPOUT_KEY],\n","            bi_directional = config_defaults[IS_BIDIRECTIONAL_KEY]\n","            )\n","        decoder1 = DecoderRNN(\n","            embedding_size = config_defaults[EMBEDDING_SIZE_KEY], \n","            hidden_size = config_defaults[HIDDEN_LAYER_KEY],\n","            num_layers_decoder = config_defaults[ENCODER_LAYER_KEY],\n","            cell_type = config_defaults[CELL_TYPE_KEY],\n","            drop_out = config_defaults[DROPOUT_KEY],\n","            bi_directional = config_defaults[IS_BIDIRECTIONAL_KEY], \n","            output_size = output_langs.n_chars\n","            )\n","\n","        if is_gpu:\n","            encoder1, decoder1 = encoder1.cuda(), decoder1.cuda()\n","\n","        print(\"vanilla seq2seqWithoutAttn\")\n","        # Train and evaluate the Seq2SeqWithoutAttn model\n","        seq2seq(\n","            encoder = encoder1,\n","            decoder = decoder1,\n","            train_loader = train_loader,\n","            val_loader = val_loader,\n","            test_loader = test_loader,\n","            lr = config_defaults[LEARNING_RATE_KEY],\n","            optimizer = optimizer,\n","            epochs = config_defaults[EPOCHS_KEY],\n","            max_length_word = max_len,\n","            num_layers_enc = config_defaults[ENCODER_LAYER_KEY],\n","            output_lang = output_langs,\n","            batch_size = config_defaults[BATCH_SIZE_KEY],\n","            cell_type = config_defaults[CELL_TYPE_KEY],\n","            is_wandb = is_wandb\n","            )\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_id = wandb.sweep(sweep_config, project=\"dl-assignment-3\", entity=\"cs23m007\")\n","print('sweep_id: ', sweep_id)\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4830499,"sourceId":8163901,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
