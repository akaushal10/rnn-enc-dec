{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8163901,"sourceType":"datasetVersion","datasetId":4830499}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport wandb\nimport torch\nimport torch.nn as nn\nimport random\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport pandas as pd\nimport torch.optim as optim\nimport torch.nn.functional as Function\nimport argparse\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T07:12:33.187191Z","iopub.execute_input":"2024-04-24T07:12:33.187567Z","iopub.status.idle":"2024-04-24T07:12:38.671156Z","shell.execute_reply.started":"2024-04-24T07:12:33.187539Z","shell.execute_reply":"2024-04-24T07:12:38.670194Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"SYMBOL_BEGIN, SYMBOL_END, SYMBOL_UNKNOWN, SYMBOL_PADDING = 0, 1, 2, 3\n\nINPUT_LABEL = \"input\"\nTARGET_LABEL = \"target\"\nDELIMETER = \",\"\n\nRNN_KEY = \"RNN\"\nGRU_KEY = \"GRU\"\nLSTM_KEY = \"LSTM\"\n\nINPUT_LANG_KEY = \"input_lang\"\nOUTPUT_LANG_KEY = \"output_lang\"\nPAIRS_KEY = \"pairs\"\nMAX_LEN_KEY = \"max_len\"\n\ninput_lang = \"eng\"\nTARGET_LANG = \"hin\"\n\nTRAIN_LABEL = \"train\"\nTEST_LABEL = \"test\"\nVALID_LABEL = \"valid\"\n\nDEFAULT_PATH = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled\"\nTRAIN_DATASET_PATH = f\"{DEFAULT_PATH}/{TARGET_LANG}/{TARGET_LANG}_{TRAIN_LABEL}.csv\"\nVALIDATION_DATASET_PATH = f\"{DEFAULT_PATH}/{TARGET_LANG}/{TARGET_LANG}_{VALID_LABEL}.csv\"\nTEST_DATASET_PATH = f\"{DEFAULT_PATH}/{TARGET_LANG}/{TARGET_LANG}_{TEST_LABEL}.csv\"\n\nNADAM_KEY = \"Nadam\"\n\n\nis_gpu = torch.cuda.is_available()\n\n\n# Set the device type to CUDA if available, otherwise use CPU\nif is_gpu:\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T07:14:11.799999Z","iopub.execute_input":"2024-04-24T07:14:11.800650Z","iopub.status.idle":"2024-04-24T07:14:11.828616Z","shell.execute_reply.started":"2024-04-24T07:14:11.800598Z","shell.execute_reply":"2024-04-24T07:14:11.827611Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self):\n        self.str_count,self.int_encodding = dict(),dict()\n        self.n_chars = 4\n        self.str_encodding = {0: \"<\", 1: \">\", 2: \"?\", 3: \".\"}\n\n    def addWord(self, word):\n        for char in word:\n            try:\n                self.str_count[char] += 1\n            except:\n                self.int_encodding[char] = self.n_chars\n                self.str_encodding[self.n_chars] = char\n                self.str_count[char] = 1\n                self.n_chars += 1\n\n# prepareDataWithoutAttn\ndef prepareData(dir):\n    data = pd.read_csv(dir, sep=DELIMETER, names=[INPUT_LABEL, TARGET_LABEL])\n\n    max_input_length = data[INPUT_LABEL].apply(len).max()\n    max_target_length = data[TARGET_LABEL].apply(len).max()\n    \n    max_len=max(max_input_length,max_target_length)\n\n    input_lang, output_lang = Vocabulary(), Vocabulary()\n\n    pairs = pd.concat([data[INPUT_LABEL], data[TARGET_LABEL]], axis=1).values.tolist()\n\n    for pair in pairs:\n        input_lang.addWord(pair[0])\n        output_lang.addWord(pair[1])\n\n    return input_lang,output_lang,pairs,max_len\n\n# helpTensorWithoutAttn\ndef helpTensor(lang, word, max_length):\n    index_list = []\n    for char in word:\n        try:\n            index_list.append(lang.char2index[char])\n        except:\n            index_list.append(SYMBOL_UNKNOWN)\n\n    indexes = index_list\n    indexes.append(SYMBOL_END)\n    n = len(indexes)\n    indexes.extend([SYMBOL_PADDING] * (max_length - n))\n    result = torch.LongTensor(indexes)\n    if is_gpu:\n        return result.cuda()\n    return result\n\n\n# MakeTensorWithoutAttn\ndef makeTensor(input_lang, output_lang, pairs, reach):\n    res = []\n    for i in range(len(pairs)):\n        # Convert input and target sequences to tensors using the helpTensorWithoutAttn function\n        input_variable = helpTensor(input_lang, pairs[i][0], reach)\n        target_variable = helpTensor(output_lang, pairs[i][1], reach)\n        res.append((input_variable, target_variable))\n    return res\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T07:14:13.653678Z","iopub.execute_input":"2024-04-24T07:14:13.654369Z","iopub.status.idle":"2024-04-24T07:14:13.668777Z","shell.execute_reply.started":"2024-04-24T07:14:13.654332Z","shell.execute_reply":"2024-04-24T07:14:13.667773Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# accuracyWithoutAttn\ndef accuracy(encoder, decoder, loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang):\n    with torch.no_grad():\n        total = 0\n        correct = 0\n\n        for batch_x, batch_y in loader:\n            # Initialize encoder hidden state\n            encoder_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n            input_variable = Variable(batch_x.transpose(0, 1))\n            target_variable = Variable(batch_y.transpose(0, 1))\n\n            # Check if LSTM and initialize cell state\n            if cell_type == \"LSTM\":\n                encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n                encoder_hidden = (encoder_hidden, encoder_cell_state)\n\n            # input_length = input_variable.size()[0]\n            # target_length = target_variable.size()[0]\n\n            output = torch.LongTensor(target_variable.size()[0], batch_size)\n\n            # Initialize encoder outputs\n            # encoder_outputs = Variable(torch.zeros(max_length, batch_size, encoder.hidden_size))\n            # encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n\n            # Encoder forward pass\n            for ei in range(input_variable.size()[0]):\n                encoder_hidden = encoder(input_variable[ei], batch_size, encoder_hidden)[1]\n\n            decoder_input = Variable(torch.LongTensor([SYMBOL_BEGIN] * batch_size))\n            decoder_input = decoder_input.cuda() if is_gpu else decoder_input\n\n            decoder_hidden = encoder_hidden\n\n            # Decoder forward pass\n            for di in range(target_variable.size()[0]):\n                decoder_output, decoder_hidden = decoder(decoder_input, batch_size, decoder_hidden)\n                topi = decoder_output.data.topk(1)[1]\n                output[di] = torch.cat(tuple(topi))\n                decoder_input = torch.cat(tuple(topi))\n\n            output = output.transpose(0, 1)\n\n            # Calculate accuracyWithoutAttn\n            for di in range(output.size()[0]):\n                ignore = [SYMBOL_BEGIN, SYMBOL_END, SYMBOL_PADDING]\n                sent = [output_lang.str_encodding[letter.item()] for letter in output[di] if letter not in ignore]\n                y = [output_lang.str_encodding[letter.item()] for letter in batch_y[di] if letter not in ignore]\n                if sent == y:\n                    correct += 1\n                total += 1\n\n    return (correct / total) * 100\n\n\n# calc_lossWithoutAttn\ndef calc_loss(encoder, decoder, input_tensor, target_tensor, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training, teacher_forcing_ratio=0.5):\n    # Initialize the encoder hidden state\n    output_hidden = encoder.initHidden(batch_size, num_layers_enc)\n\n    # Check if LSTM and initialize cell state\n    if cell_type == LSTM_KEY:\n        encoder_cell_state = encoder.initHidden(batch_size, num_layers_enc)\n        output_hidden = (output_hidden, encoder_cell_state)\n\n    # Zero the gradients\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    # Get input and target sequence lengths\n    # input_length = input_tensor.size(0)\n    # target_length = target_tensor.size(0)\n\n    # Initialize loss\n    loss = 0\n\n    # Encoder forward pass\n    for ei in range(input_tensor.size(0)):\n        output_hidden = encoder(input_tensor[ei], batch_size, output_hidden)[1]\n\n    # Initialize decoder input\n    decoder_input = torch.LongTensor([SYMBOL_BEGIN] * batch_size)\n    decoder_input = decoder_input.cuda() if is_gpu else decoder_input\n\n    # Determine if using teacher forcing\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    # Loop over target sequence\n    if is_training:\n        # Training phase\n        for di in range(target_tensor.size(0)):\n            decoder_output, output_hidden = decoder(decoder_input, batch_size, output_hidden)\n            decoder_input = target_tensor[di] if use_teacher_forcing else decoder_output.argmax(dim=1)\n            loss = criterion(decoder_output, target_tensor[di]) + loss\n    else:\n        # Validation phase\n        with torch.no_grad():\n            for di in range(target_tensor.size(0)):\n                decoder_output, output_hidden = decoder(decoder_input, batch_size, output_hidden)\n                loss += criterion(decoder_output, target_tensor[di])\n                decoder_input = decoder_output.argmax(dim=1)\n\n    # Backpropagation and optimization in training phase\n    if is_training:\n        loss.backward()\n        encoder_optimizer.step()\n        decoder_optimizer.step()\n\n    # Return the average loss per target length\n    return loss.item() / target_tensor.size(0)\n\n\n# Train and evaluate the Seq2SeqWithoutAttn model\ndef seq2seq(encoder, decoder, train_loader, val_loader, test_loader, lr, optimizer, epochs, max_length_word, num_layers_enc, output_lang,batch_size,cell_type):\n    max_length = max_length_word - 1\n    # Define the optimizer and criterion\n    encoder_optimizer = optim.NAdam(encoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(encoder.parameters(), lr=lr)\n    decoder_optimizer = optim.NAdam(decoder.parameters(), lr=lr) if optimizer == \"nadam\" else optim.Adam(decoder.parameters(), lr=lr)\n    criterion = nn.NLLLoss()\n\n    for epoch in range(epochs):\n        train_loss_total = 0\n        val_loss_total = 0\n\n        # Training phase\n        for batch_x, batch_y in train_loader:\n            batch_x = Variable(batch_x.transpose(0, 1))\n            batch_y = Variable(batch_y.transpose(0, 1))\n            # Calculate the training loss\n            loss = calc_loss(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=True)\n            train_loss_total += loss\n\n        train_loss_avg = train_loss_total / len(train_loader)\n        print(f\"Epoch: {epoch} | Train Loss: {train_loss_avg:.4f} |\", end=\"\")\n\n        # Validation phase\n        for batch_x, batch_y in val_loader:\n            batch_x = Variable(batch_x.transpose(0, 1))\n            batch_y = Variable(batch_y.transpose(0, 1))\n            # Calculate the validation loss\n            loss = calc_loss(encoder, decoder, batch_x, batch_y, batch_size, encoder_optimizer, decoder_optimizer, criterion, cell_type, num_layers_enc, max_length, is_training=False)\n            val_loss_total += loss\n\n        val_loss_avg = val_loss_total / len(val_loader)\n        print(f\"Val Loss: {val_loss_avg:.4f} |\", end=\"\")\n\n        # Calculate validation accuracyWithoutAttn\n        val_acc = accuracy(encoder, decoder, val_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n        val_acc /= 100\n        print(f\"Val Accuracy: {val_acc:.4%}\")\n        \n        if epochs-1==epoch :\n            test_acc = accuracy(encoder, decoder, test_loader, batch_size, criterion, cell_type, num_layers_enc, max_length, output_lang)\n            test_acc /= 100\n            print(f\"Test Accuracy: {test_acc:.4%}\")\n            \n\n\n\n# EncoderRNNWithoutAttn\nclass EncoderRNN(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers_encoder, cell_type, drop_out, bi_directional):\n        super(EncoderRNN, self).__init__()\n\n        self.emb_n = embedding_size\n        self.hid_n = hidden_size\n        self.encoder_n = num_layers_encoder\n        self.model_key = cell_type\n        self.is_dropout = drop_out\n        self.is_bi_dir = bi_directional\n\n        self.embedding = nn.Embedding(input_size, self.emb_n)\n        self.dropout = nn.Dropout(self.is_dropout)\n\n        cell_map = dict({RNN_KEY: nn.RNN, GRU_KEY: nn.GRU, LSTM_KEY: nn.LSTM})\n        self.cell_layer = cell_map[self.model_key](\n            input_size = self.emb_n,\n            hidden_size = self.hid_n,\n            num_layers=self.encoder_n,\n            dropout=self.is_dropout,\n            bidirectional=self.is_bi_dir,\n        )\n\n    def forward(self, input, batch_size, hidden):\n        embedded = self.dropout(self.embedding(input).view(1, batch_size, -1))\n\n        output, hidden = self.cell_layer(embedded, hidden)\n        return output, hidden\n\n    def initHidden(self, batch_size, num_layers_enc):\n        if self.is_bi_dir:\n            weights = torch.zeros(num_layers_enc * 2 , batch_size, self.hid_n)\n        else:\n            weights = torch.zeros(num_layers_enc, batch_size, self.hid_n)\n\n        if is_gpu:\n            return weights.cuda()\n        return weights\n    \n# DecoderRNNWithoutAttn\nclass DecoderRNN(nn.Module):\n    def __init__(self, embedding_size, hidden_size, num_layers_decoder, cell_type, drop_out, bi_directional, output_size):\n        super(DecoderRNN, self).__init__()\n\n        self.emb_n = embedding_size\n        self.hid_n = hidden_size\n        self.decoder_n = num_layers_decoder\n        self.model_key = cell_type\n        self.is_dropout = drop_out\n        self.is_bi_dir = bi_directional\n\n        # Create an embedding layer\n        self.embedding = nn.Embedding(output_size, self.emb_n)\n        self.dropout = nn.Dropout(self.is_dropout)\n\n        cell_map = {RNN_KEY: nn.RNN, GRU_KEY: nn.GRU, LSTM_KEY: nn.LSTM}\n        self.cell_layer = cell_map[self.model_key](\n            input_size = self.emb_n,\n            hidden_size = self.hid_n,\n            num_layers=self.decoder_n,\n            dropout=self.is_dropout,\n            bidirectional=self.is_bi_dir,\n        )\n\n        # Linear layer for output\n        if self.is_bi_dir :\n            self.out = nn.Linear(self.hid_n * 2, output_size)\n        else:\n            self.out = nn.Linear(self.hid_n,output_size)\n\n        # Softmax activation\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, batch_size, hidden):\n        output = Function.relu(self.dropout(self.embedding(input).view(1, batch_size, -1)))\n        output, hidden = self.cell_layer(output, hidden)\n\n        output = self.softmax(self.out(output[0]))\n        return output, hidden\n\n\ndef train(flag):\n    optimizer = NADAM_KEY\n    alpha = 0.001\n    hl_size = 256\n    model_key = LSTM_KEY\n    encoder_n = 2\n    num_layers_decoder = 2\n    dropout_val = 0.2\n    epochs = 5\n    embedding_size = 256\n    is_bi_dir = False\n    batch_size = 32\n\n    if flag:\n        pass\n    else:\n        # Prepare training data\n        input_langs,output_langs,pairs,max_len = prepareData(TRAIN_DATASET_PATH)\n        print(\"train:sample:\", random.choice(pairs))\n        train_n = len(pairs)\n        print(f\"Number of training examples: {train_n}\")\n\n        # Prepare validation data\n        input_langs,output_langs,val_pairs,max_len_val = prepareData(VALIDATION_DATASET_PATH)\n        val_n = len(val_pairs)\n        print(\"validation:sample:\", random.choice(val_pairs))\n        print(f\"Number of validation examples: {val_n}\")\n\n        # Prepare test data\n        input_langs,output_langs,test_pairs,max_len_test = prepareData(TEST_DATASET_PATH)\n        test_n = len(test_pairs)\n        print(\"Test:sample:\", random.choice(test_pairs))\n        print(f\"Number of Test examples: {test_n}\")\n\n        max_len = max(max_len, max(max_len_val, max_len_test)) + 4\n        print(max_len)\n\n        # Convert data to tensors and create data loaders\n        pairs = makeTensor(input_langs, output_langs, pairs, max_len)\n        val_pairs = makeTensor(input_langs, output_langs, val_pairs, max_len)\n        test_pairs = makeTensor(input_langs, output_langs, test_pairs, max_len)\n\n        train_loader = DataLoader(dataset = pairs, batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(dataset = val_pairs, batch_size=batch_size, shuffle=True)\n        test_loader = DataLoader(dataset = test_pairs, batch_size=batch_size, shuffle=True)\n\n        # Create the encoder and decoder models\n        encoder1 = EncoderRNN(input_langs.n_chars, embedding_size, hl_size, encoder_n, model_key, dropout_val, is_bi_dir)\n        decoder1 = DecoderRNN(embedding_size, hl_size, encoder_n, model_key, dropout_val, is_bi_dir, output_langs.n_chars)\n\n        if is_gpu:\n            encoder1, decoder1 = encoder1.cuda(), decoder1.cuda()\n\n        print(\"vanilla seq2seqWithoutAttn\")\n        # Train and evaluate the Seq2SeqWithoutAttn model\n        seq2seq(encoder1, decoder1, train_loader, val_loader, test_loader, alpha, optimizer, epochs, max_len, encoder_n, output_langs, batch_size = 32, cell_type = model_key)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-24T07:20:36.782044Z","iopub.execute_input":"2024-04-24T07:20:36.782415Z","iopub.status.idle":"2024-04-24T07:20:36.837554Z","shell.execute_reply.started":"2024-04-24T07:20:36.782386Z","shell.execute_reply":"2024-04-24T07:20:36.836502Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train(False)","metadata":{"execution":{"iopub.status.busy":"2024-04-24T07:20:37.836244Z","iopub.execute_input":"2024-04-24T07:20:37.836608Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"train:sample: ['devoki', 'देवोकी']\nNumber of training examples: 51200\nvalidation:sample: ['kothiyalmadhu', 'कोठियालमधु']\nNumber of validation examples: 4096\nTest:sample: ['qiwi', 'क्यूआईडब्ल्यूआई']\nNumber of Test examples: 4096\n30\nvanilla seq2seqWithoutAttn\nEpoch: 0 | Train Loss: 0.1396 |Val Loss: 0.1738 |Val Accuracy: 21.9238%\nEpoch: 1 | Train Loss: 0.1167 |Val Loss: 0.1601 |Val Accuracy: 18.0908%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}